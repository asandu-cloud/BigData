{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bd3fa3a",
   "metadata": {},
   "source": [
    "## Importing all dependencies for the Notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe686ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import findspark\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, count, min as spark_min, max as spark_max,\n",
    "    avg, length, to_timestamp, expr, to_date, sum as spark_sum\n",
    ")\n",
    "import time \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee899f3",
   "metadata": {},
   "source": [
    "## Initializing Java Home for Spark\n",
    "- Notebook setup, change path to path of local JAVA HOME install\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afa1143",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Python executable:\", sys.executable)\n",
    "print(\"Python version   :\", sys.version)\n",
    "print(\"JAVA_HOME before :\", os.environ.get(\"JAVA_HOME\"))\n",
    "\n",
    "# JAVA_HOME = \"/opt/homebrew/Cellar/openjdk@11/11.0.29/libexec/openjdk.jdk/Contents/Home\" # andrei \n",
    "# JAVA_HOME=\"/Library/Java/JavaVirtualMachines/temurin-11.jdk/Contents/Home\" # ezgim\n",
    "os.environ[\"JAVA_HOME\"] = JAVA_HOME\n",
    "os.environ[\"PATH\"] = os.path.join(JAVA_HOME, \"bin\") + \":\" + os.environ[\"PATH\"]\n",
    "\n",
    "print(\"JAVA_HOME after  :\", os.environ.get(\"JAVA_HOME\"))\n",
    "\n",
    "import subprocess\n",
    "print(\"\\njava -version from this kernel:\")\n",
    "print(subprocess.check_output([\"java\", \"-version\"], stderr=subprocess.STDOUT).decode())\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .appName(\"test_jvm\")\n",
    "        .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"\\nSpark version:\", spark.version)\n",
    "\n",
    "# Tiny test job\n",
    "spark.range(5).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5485b4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7860e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    project_root = Path(\n",
    "        subprocess.check_output([\"git\", \"rev-parse\", \"--show-toplevel\"])\n",
    "        .decode()\n",
    "        .strip()\n",
    "    )\n",
    "except Exception:\n",
    "    project_root = Path.cwd().parent\n",
    "\n",
    "print(\"PROJECT ROOT:\", project_root)\n",
    "\n",
    "raw_dir = project_root / \"data\" / \"raw\"\n",
    "print(\"RAW DATA DIR:\", raw_dir)\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .appName(\"IRA_Tweets_Task1\")\n",
    "        .master(\"local[*]\")\n",
    "        .config(\"spark.driver.memory\", \"6g\")      # adjust down/up depending on your RAM\n",
    "        .config(\"spark.executor.memory\", \"6g\")    # local = same as driver\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"4\")\n",
    "        .getOrCreate()\n",
    ")\n",
    "\n",
    "df = (\n",
    "    spark.read\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .option(\"multiLine\", \"true\")\n",
    "        .option(\"escape\", \"\\\"\")\n",
    "        .csv(str(raw_dir))\n",
    ")\n",
    "\n",
    "df.printSchema()\n",
    "print(\"Total rows:\", df.count())\n",
    "\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff66e12",
   "metadata": {},
   "source": [
    "## Exploratory analysis \n",
    "- Exploring the dataset by looking for:\n",
    "    - Total number of tweets\n",
    "    - Total number of unique accounts\n",
    "    - Average tweets per account\n",
    "    - Media tweets per account\n",
    "    - Max tweets by a single amount\n",
    "    - Std dev of tweets per account\n",
    "    - Mean account lifespan (days)\n",
    "    - Average tweet length (characters)\n",
    "    - Average followers per account "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843b9c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# timestamp column\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "df_time = df.withColumn(\n",
    "    \"publish_ts\",\n",
    "    to_timestamp(\"publish_date\", \"M/d/yyyy H:mm\")\n",
    ")\n",
    "\n",
    "# global accounts\n",
    "total_tweets = df.count()\n",
    "n_accounts = df.selectExpr(\"count(distinct author) as n\").first()[\"n\"]\n",
    "\n",
    "# / account stats\n",
    "acct_stats_spark = (\n",
    "    df_time.groupBy(\"author\")\n",
    "           .agg(\n",
    "               count(\"*\").alias(\"n_tweets\"),\n",
    "               spark_min(\"publish_ts\").alias(\"first_ts\"),\n",
    "               spark_max(\"publish_ts\").alias(\"last_ts\"),\n",
    "               avg(\"followers\").alias(\"avg_followers\")\n",
    "           )\n",
    ")\n",
    "\n",
    "acct_stats = acct_stats_spark.toPandas()\n",
    "\n",
    "# lifespan\n",
    "acct_stats[\"lifespan_days\"] = (\n",
    "    (acct_stats[\"last_ts\"] - acct_stats[\"first_ts\"])\n",
    "    .dt.total_seconds() / 86400.0\n",
    ").clip(lower=0)\n",
    "\n",
    "# acc level metrics\n",
    "avg_tweets = acct_stats[\"n_tweets\"].mean()\n",
    "median_tweets = acct_stats[\"n_tweets\"].median()\n",
    "max_tweets = acct_stats[\"n_tweets\"].max()\n",
    "std_tweets = acct_stats[\"n_tweets\"].std()\n",
    "\n",
    "mean_lifespan = acct_stats[\"lifespan_days\"].mean()\n",
    "avg_followers_per_account = acct_stats[\"avg_followers\"].mean()\n",
    "\n",
    "# avg tweet length \n",
    "avg_tweet_length = (\n",
    "    df.select(avg(length(\"content\")).alias(\"avg_len\"))\n",
    "      .first()[\"avg_len\"]\n",
    ")\n",
    "\n",
    "# defining stats table \n",
    "stats_df = pd.DataFrame({\n",
    "    \"Metric\": [\n",
    "        \"Total number of tweets\",\n",
    "        \"Total number of unique accounts\",\n",
    "        \"Average tweets per account\",\n",
    "        \"Median tweets per account\",\n",
    "        \"Max tweets by a single account\",\n",
    "        \"Std dev of tweets per account\",\n",
    "        \"Mean account lifespan (days)\",\n",
    "        \"Average tweet length (characters)\",\n",
    "        \"Average followers per account\",\n",
    "    ],\n",
    "    \"Value\": [\n",
    "        f\"{total_tweets:,}\",\n",
    "        f\"{n_accounts:,}\",\n",
    "        f\"{avg_tweets:.2f}\",\n",
    "        f\"{median_tweets:.2f}\",\n",
    "        f\"{max_tweets:,}\",\n",
    "        f\"{std_tweets:.2f}\",\n",
    "        f\"{mean_lifespan:.2f}\",\n",
    "        f\"{avg_tweet_length:.2f}\",\n",
    "        f\"{avg_followers_per_account:.2f}\",\n",
    "    ]\n",
    "})\n",
    "\n",
    "plt.close('all')\n",
    "\n",
    "# plt \n",
    "fig, ax = plt.subplots(figsize=(9, 3))\n",
    "ax.axis(\"off\")\n",
    "\n",
    "# add title above table\n",
    "fig, ax = plt.subplots(figsize=(9, 3))\n",
    "\n",
    "# move table lower\n",
    "ax.set_position([0, -0.05, 1, 1])\n",
    "ax.axis(\"off\")\n",
    "\n",
    "plt.title(\"Key Dataset Statistics\", pad=20)\n",
    "\n",
    "table = ax.table(\n",
    "    cellText=stats_df.values,\n",
    "    colLabels=stats_df.columns,\n",
    "    loc=\"center\",\n",
    "    cellLoc=\"center\"\n",
    ")\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(11)\n",
    "table.scale(1, 2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34866dc",
   "metadata": {},
   "source": [
    "## Investigating activity of accounts belonging to different categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418d01de",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "df_time = df.withColumn(\n",
    "    \"publish_ts\",\n",
    "    to_timestamp(\"publish_date\", \"M/d/yyyy H:mm\")  # handles 1/2/2017 14:39 etc.\n",
    ")\n",
    "\n",
    "# tweets per month by account category \n",
    "df_cat_monthly = (\n",
    "    df_time\n",
    "        .where(col(\"publish_ts\").isNotNull())\n",
    "        .groupBy(\n",
    "            year(\"publish_ts\").alias(\"year\"),\n",
    "            month(\"publish_ts\").alias(\"month\"),\n",
    "            col(\"account_category\")\n",
    "        )\n",
    "        .count()\n",
    ")\n",
    "\n",
    "df_cat_monthly.show(10, truncate=False)\n",
    "\n",
    "# top n categories by tweet volume \n",
    "cat_totals = (\n",
    "    df_cat_monthly\n",
    "        .groupBy(\"account_category\")\n",
    "        .agg(spark_sum(\"count\").alias(\"total_tweets\"))\n",
    "        .orderBy(col(\"total_tweets\").desc())\n",
    ")\n",
    "\n",
    "cat_totals.show(truncate=False)\n",
    "\n",
    "# choose top 4â€“5 categories for a clean plot\n",
    "TOP_N = 5\n",
    "top_categories = [\n",
    "    row[\"account_category\"]\n",
    "    for row in cat_totals.limit(TOP_N).collect()\n",
    "    if row[\"account_category\"] is not None\n",
    "]\n",
    "\n",
    "print(\"Top categories to plot:\", top_categories)\n",
    "\n",
    "# monthly data convert to pandas \n",
    "cat_monthly_pd = df_cat_monthly.toPandas()\n",
    "\n",
    "# filter to top categories only\n",
    "cat_monthly_pd = cat_monthly_pd[cat_monthly_pd[\"account_category\"].isin(top_categories)]\n",
    "\n",
    "# build a proper year_month datetime for x-axis\n",
    "cat_monthly_pd[\"year_month\"] = pd.to_datetime(\n",
    "    cat_monthly_pd[\"year\"].astype(str) + \"-\" +\n",
    "    cat_monthly_pd[\"month\"].astype(str).str.zfill(2) + \"-01\"\n",
    ")\n",
    "\n",
    "# pivot: rows = year_month, columns = account_category, values = tweet counts\n",
    "pivot = cat_monthly_pd.pivot_table(\n",
    "    index=\"year_month\",\n",
    "    columns=\"account_category\",\n",
    "    values=\"count\",\n",
    "    aggfunc=\"sum\",\n",
    "    fill_value=0\n",
    ").sort_index()\n",
    "\n",
    "pivot.head()\n",
    "\n",
    "# Tweets over time by account_category\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "\n",
    "for category in pivot.columns:\n",
    "    plt.plot(pivot.index, pivot[category], marker=\"o\", linewidth=1, label=category)\n",
    "\n",
    "plt.title(\"Monthly Tweet Volume by Account Category\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Number of Tweets\")\n",
    "plt.legend(title=\"Account Category\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a261cd02",
   "metadata": {},
   "source": [
    "## Tweet distribution statistics\n",
    "- Detecting spread of nunique tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c7777a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_per_account_spark = (\n",
    "    df.groupBy(\"author\")\n",
    "      .count()\n",
    "      .withColumnRenamed(\"count\", \"tweets_per_account\")\n",
    ")\n",
    "\n",
    "tweets_per_account = tweets_per_account_spark.toPandas()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(tweets_per_account[\"tweets_per_account\"], bins=50, color=\"steelblue\", edgecolor=\"black\")\n",
    "plt.title(\"Distribution of Tweets per Account (Linear Scale)\")\n",
    "plt.xlabel(\"Number of Tweets\")\n",
    "plt.ylabel(\"Number of Accounts\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(tweets_per_account[\"tweets_per_account\"], bins=50, color=\"salmon\", edgecolor=\"black\", log=True)\n",
    "plt.title(\"Distribution of Tweets per Account (Log Scale)\")\n",
    "plt.xlabel(\"Number of Tweets (log scale)\")\n",
    "plt.ylabel(\"Number of Accounts\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a7992f",
   "metadata": {},
   "source": [
    "## Detecting outliers \n",
    "- We create new smaller datasets derived from the initial dataset of accounts that are outliers \n",
    "- This is done to explore the most prominent trolls and investigate their behaviour "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07cad56",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_per_account = (\n",
    "    df.groupBy(\"author\")\n",
    "      .agg(count(\"*\").alias(\"tweets_per_account\"))\n",
    ")\n",
    "\n",
    "# high activity datasets\n",
    "accounts_over_50000 = tweets_per_account.filter(col(\"tweets_per_account\") > 50000)\n",
    "accounts_over_40000 = tweets_per_account.filter(col(\"tweets_per_account\") > 40000)\n",
    "\n",
    "\n",
    "print(\"Accounts with > 50,000 tweets:\")\n",
    "accounts_over_50000.show(truncate=False)\n",
    "\n",
    "print(\"\\nAccounts with > 40,000 tweets:\")\n",
    "accounts_over_40000.show(truncate=False)\n",
    "\n",
    "# convertion to pandas for inspection\n",
    "accounts_50k_pd = accounts_over_50000.toPandas()\n",
    "accounts_40k_pd = accounts_over_40000.toPandas()\n",
    "\n",
    "accounts_50k_pd, accounts_40k_pd\n",
    "\n",
    "# tweets per account\n",
    "tweets_per_account = (\n",
    "    df.groupBy(\"author\")\n",
    "      .agg(count(\"*\").alias(\"tweets_per_account\"))\n",
    ")\n",
    "\n",
    "# pandas for percentile calc\n",
    "tp_pd = tweets_per_account.toPandas()\n",
    "\n",
    "# # 95th percentile \n",
    "p95_threshold = tp_pd[\"tweets_per_account\"].quantile(0.95)\n",
    "\n",
    "print(f\"90th percentile tweet count threshold: {p95_threshold:.2f}\")\n",
    "\n",
    "# nunique accounts above 95th percentile\n",
    "n_above_90 = (tp_pd[\"tweets_per_account\"] > p95_threshold).sum()\n",
    "\n",
    "print(f\"Number of accounts above 95th percentile: {n_above_90}\")\n",
    "\n",
    "# show as df\n",
    "accounts_above_90 = tweets_per_account.filter(col(\"tweets_per_account\") > p95_threshold)\n",
    "accounts_above_90.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f6706f",
   "metadata": {},
   "source": [
    "## Further exploration of the outliers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3329a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_per_account = (\n",
    "    df.groupBy(\"author\")\n",
    "      .agg(count(\"*\").alias(\"tweets_per_account\"))\n",
    ")\n",
    "\n",
    "tp_pd = tweets_per_account.toPandas()\n",
    "p95_threshold = tp_pd[\"tweets_per_account\"].quantile(0.95)\n",
    "\n",
    "p95_threshold\n",
    "\n",
    "accounts_above_95 = (\n",
    "    tweets_per_account\n",
    "        .filter(col(\"tweets_per_account\") > p95_threshold)\n",
    "        .withColumnRenamed(\"author\", \"account\")\n",
    ")\n",
    "\n",
    "accounts_above_95.show(truncate=False)\n",
    "print(\"Number of accounts above 95th percentile:\", accounts_above_95.count())\n",
    "\n",
    "df_top5 = (\n",
    "    df.join(accounts_above_95, df.author == accounts_above_95.account, \"inner\")\n",
    "      .drop(accounts_above_95.account)\n",
    ")\n",
    "\n",
    "df_top5.printSchema()\n",
    "print(\"Total tweets from top 5% accounts:\", df_top5.count())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
