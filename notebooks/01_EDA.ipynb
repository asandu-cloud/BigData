{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bd3fa3a",
   "metadata": {},
   "source": [
    "## Importing all dependencies for the Notebook\n",
    "Important for running the notebook. Not necessarilly relevant to our analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe686ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import findspark\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, count, min as spark_min, max as spark_max,\n",
    "    avg, length, to_timestamp, expr, to_date, sum as spark_sum, year, month\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "import time \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee899f3",
   "metadata": {},
   "source": [
    "## PySpark setup inside of notebook. \n",
    "- Notebook initialization in the next two cells\n",
    "- Swap JAVA_HOME path into local path. Check for nano zsh.\n",
    "- Initialise `findspark` (if needed) so that PySpark can be used from the notebook.\n",
    "- Create a `SparkSession` with appropriate configuration for local development (e.g. number of cores, memory).\n",
    "- Print out basic environment information (Python and Spark versions) for reproducibility.\n",
    "- Optionally set Spark SQL configurations (e.g. time parsing policy) to match the structure of the FiveThirtyEight IRA Twitter dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afa1143",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Python executable:\", sys.executable)\n",
    "print(\"Python version   :\", sys.version)\n",
    "print(\"JAVA_HOME before :\", os.environ.get(\"JAVA_HOME\"))\n",
    "\n",
    "JAVA_HOME = \"/opt/homebrew/Cellar/openjdk@11/11.0.29/libexec/openjdk.jdk/Contents/Home\" # andrei \n",
    "# JAVA_HOME=\"/Library/Java/JavaVirtualMachines/temurin-11.jdk/Contents/Home\" # ezgim\n",
    "os.environ[\"JAVA_HOME\"] = JAVA_HOME\n",
    "os.environ[\"PATH\"] = os.path.join(JAVA_HOME, \"bin\") + \":\" + os.environ[\"PATH\"]\n",
    "\n",
    "print(\"JAVA_HOME after  :\", os.environ.get(\"JAVA_HOME\"))\n",
    "\n",
    "import subprocess\n",
    "print(\"\\njava -version from this kernel:\")\n",
    "print(subprocess.check_output([\"java\", \"-version\"], stderr=subprocess.STDOUT).decode())\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .appName(\"test_jvm\")\n",
    "        .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"\\nSpark version:\", spark.version)\n",
    "\n",
    "# Tiny test job\n",
    "spark.range(5).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7860e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    project_root = Path(\n",
    "        subprocess.check_output([\"git\", \"rev-parse\", \"--show-toplevel\"])\n",
    "        .decode()\n",
    "        .strip()\n",
    "    )\n",
    "except Exception:\n",
    "    project_root = Path.cwd().parent\n",
    "\n",
    "print(\"PROJECT ROOT:\", project_root)\n",
    "\n",
    "raw_dir = project_root / \"data\" / \"raw\"\n",
    "print(\"RAW DATA DIR:\", raw_dir)\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .appName(\"IRA_Tweets_Task1\")\n",
    "        .master(\"local[*]\")\n",
    "        .config(\"spark.driver.memory\", \"6g\")      # adjust down/up depending on your RAM\n",
    "        .config(\"spark.executor.memory\", \"6g\")    # local = same as driver\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"4\")\n",
    "        .getOrCreate()\n",
    ")\n",
    "\n",
    "df = (\n",
    "    spark.read\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .option(\"multiLine\", \"true\")\n",
    "        .option(\"escape\", \"\\\"\")\n",
    "        .csv(str(raw_dir))\n",
    ")\n",
    "\n",
    "df.printSchema()\n",
    "print(\"Total rows:\", df.count())\n",
    "\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff66e12",
   "metadata": {},
   "source": [
    "## Exploratory analysis \n",
    "We begin by quantifying the dataaset, provding a foundation for understanding how large the operation was and how intensively each account was used. \n",
    "\n",
    "- Exploring the dataset by looking for:\n",
    "    - Total number of tweets\n",
    "    - Total number of unique accounts\n",
    "    - Average tweets per account\n",
    "    - Media tweets per account\n",
    "    - Max tweets by a single amount\n",
    "    - Std dev of tweets per account\n",
    "    - Mean account lifespan (days)\n",
    "    - Average tweet length (characters)\n",
    "    - Average followers per account \n",
    "\n",
    "These statistics show the scale and shape of the operations and help us detect assymetries. The following table summarizes the main dataset priorities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843b9c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# timestamp column\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "df_time = df.withColumn(\n",
    "    \"publish_ts\",\n",
    "    to_timestamp(\"publish_date\", \"M/d/yyyy H:mm\")\n",
    ")\n",
    "\n",
    "# global accounts\n",
    "total_tweets = df.count()\n",
    "n_accounts = df.selectExpr(\"count(distinct author) as n\").first()[\"n\"]\n",
    "\n",
    "# / account stats\n",
    "acct_stats_spark = (\n",
    "    df_time.groupBy(\"author\")\n",
    "           .agg(\n",
    "               count(\"*\").alias(\"n_tweets\"),\n",
    "               spark_min(\"publish_ts\").alias(\"first_ts\"),\n",
    "               spark_max(\"publish_ts\").alias(\"last_ts\"),\n",
    "               avg(\"followers\").alias(\"avg_followers\")\n",
    "           )\n",
    ")\n",
    "\n",
    "acct_stats = acct_stats_spark.toPandas()\n",
    "\n",
    "# lifespan\n",
    "acct_stats[\"lifespan_days\"] = (\n",
    "    (acct_stats[\"last_ts\"] - acct_stats[\"first_ts\"])\n",
    "    .dt.total_seconds() / 86400.0\n",
    ").clip(lower=0)\n",
    "\n",
    "# acc level metrics\n",
    "avg_tweets = acct_stats[\"n_tweets\"].mean()\n",
    "median_tweets = acct_stats[\"n_tweets\"].median()\n",
    "max_tweets = acct_stats[\"n_tweets\"].max()\n",
    "std_tweets = acct_stats[\"n_tweets\"].std()\n",
    "\n",
    "mean_lifespan = acct_stats[\"lifespan_days\"].mean()\n",
    "avg_followers_per_account = acct_stats[\"avg_followers\"].mean()\n",
    "\n",
    "# avg tweet length \n",
    "avg_tweet_length = (\n",
    "    df.select(avg(length(\"content\")).alias(\"avg_len\"))\n",
    "      .first()[\"avg_len\"]\n",
    ")\n",
    "\n",
    "# defining stats table \n",
    "stats_df = pd.DataFrame({\n",
    "    \"Metric\": [\n",
    "        \"Total number of tweets\",\n",
    "        \"Total number of unique accounts\",\n",
    "        \"Average tweets per account\",\n",
    "        \"Median tweets per account\",\n",
    "        \"Max tweets by a single account\",\n",
    "        \"Std dev of tweets per account\",\n",
    "        \"Mean account lifespan (days)\",\n",
    "        \"Average tweet length (characters)\",\n",
    "        \"Average followers per account\",\n",
    "    ],\n",
    "    \"Value\": [\n",
    "        f\"{total_tweets:,}\",\n",
    "        f\"{n_accounts:,}\",\n",
    "        f\"{avg_tweets:.2f}\",\n",
    "        f\"{median_tweets:.2f}\",\n",
    "        f\"{max_tweets:,}\",\n",
    "        f\"{std_tweets:.2f}\",\n",
    "        f\"{mean_lifespan:.2f}\",\n",
    "        f\"{avg_tweet_length:.2f}\",\n",
    "        f\"{avg_followers_per_account:.2f}\",\n",
    "    ]\n",
    "})\n",
    "\n",
    "plt.close('all')\n",
    "\n",
    "# plt \n",
    "fig, ax = plt.subplots(figsize=(9, 3))\n",
    "ax.axis(\"off\")\n",
    "\n",
    "# add title above table\n",
    "fig, ax = plt.subplots(figsize=(9, 3))\n",
    "\n",
    "# move table lower\n",
    "ax.set_position([0, -0.05, 1, 1])\n",
    "ax.axis(\"off\")\n",
    "\n",
    "plt.title(\"Key Dataset Statistics\", pad=20)\n",
    "\n",
    "table = ax.table(\n",
    "    cellText=stats_df.values,\n",
    "    colLabels=stats_df.columns,\n",
    "    loc=\"center\",\n",
    "    cellLoc=\"center\"\n",
    ")\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(11)\n",
    "table.scale(1, 2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e976e0e",
   "metadata": {},
   "source": [
    "## Temporal Analysis: \n",
    "\n",
    "In this part we focus on the **time dimension** of the tweets:\n",
    "\n",
    "- Convert raw timestamp columns into proper **Spark timestamp / date types**.\n",
    "- Extract temporal features such as **year, month, day, hour**, and potentially **weekday/weekend**.\n",
    "- Aggregate tweet counts over time (e.g. per day, month, or year) to visualise **activity trends**.\n",
    "\n",
    "The plots produced here help us answer questions such as:\n",
    "- When were these accounts most active?\n",
    "- Are there visible bursts or campaigns in specific periods?\n",
    "- How does activity evolve before and after key time points?\n",
    "\n",
    "These insights are important for later framing of the network and content analyses inside the presentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bbfba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp, to_date, hour, date_format\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# timestamps\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "df_time = df.withColumn(\n",
    "    \"publish_ts\",\n",
    "    to_timestamp(\"publish_date\", \"M/d/yyyy H:mm\")\n",
    ")\n",
    "\n",
    "# keep only tweets from 2014 onwards\n",
    "df_time = (\n",
    "    df_time\n",
    "        .filter(df_time.publish_ts.isNotNull())\n",
    "        .filter(df_time.publish_ts >= \"2014-01-01\")\n",
    ")\n",
    "\n",
    "# daily tweet counts\n",
    "df_daily = (\n",
    "    df_time\n",
    "        .withColumn(\"date\", to_date(\"publish_ts\"))\n",
    "        .groupBy(\"date\")\n",
    "        .count()\n",
    "        .orderBy(\"date\")\n",
    ")\n",
    "\n",
    "daily_pd = df_daily.toPandas()\n",
    "daily_pd[\"date\"] = pd.to_datetime(daily_pd[\"date\"])\n",
    "daily_pd = daily_pd.sort_values(\"date\")\n",
    "\n",
    "# 30-day rolling avg for second plot\n",
    "daily_pd[\"rolling_30\"] = daily_pd[\"count\"].rolling(30, center=True).mean()\n",
    "\n",
    "# load events csv\n",
    "events_path = \"/Users/sanduandrei/Desktop/BigData/data/events/events_temporal_markers.csv\"\n",
    "events_pd = pd.read_csv(events_path)\n",
    "events_pd[\"event_date\"] = pd.to_datetime(events_pd[\"date\"], format=\"%m/%d/%Y\")\n",
    "\n",
    "# daily plot for tweet count\n",
    "fig, ax = plt.subplots(figsize=(18, 6))\n",
    "ax.plot(daily_pd[\"date\"], daily_pd[\"count\"], linewidth=1.5, color=\"black\", label=\"Daily tweets\")\n",
    "\n",
    "colors = plt.cm.tab20.colors\n",
    "for idx, (_, row) in enumerate(events_pd.iterrows()):\n",
    "    ax.axvline(\n",
    "        row[\"event_date\"],\n",
    "        color=colors[idx % len(colors)],\n",
    "        linestyle=\"--\",\n",
    "        linewidth=1.5,\n",
    "        alpha=0.9,\n",
    "        label=row[\"label\"]\n",
    "    )\n",
    "\n",
    "ax.set_title(\"Daily Tweet Volume Over Time with Key Events Marked (2014+)\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Number of Tweets\")\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\", fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 30 day rolling average \n",
    "fig, ax = plt.subplots(figsize=(18, 6))\n",
    "ax.plot(daily_pd[\"date\"], daily_pd[\"rolling_30\"], linewidth=2, color=\"darkred\", label=\"30-day Rolling Avg\")\n",
    "\n",
    "for idx, (_, row) in enumerate(events_pd.iterrows()):\n",
    "    ax.axvline(\n",
    "        row[\"event_date\"],\n",
    "        color=colors[idx % len(colors)],\n",
    "        linestyle=\"--\",\n",
    "        linewidth=1.5,\n",
    "        alpha=0.9,\n",
    "        label=row[\"label\"]\n",
    "    )\n",
    "\n",
    "ax.set_title(\"30-Day Rolling Average of Tweet Volume with Key Events Marked (2014+)\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Tweets (Smoothed Trend)\")\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\", fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# patterns by hour of the day\n",
    "df_hourly = (\n",
    "    df_time\n",
    "        .withColumn(\"hour\", hour(\"publish_ts\"))\n",
    "        .groupBy(\"hour\")\n",
    "        .count()\n",
    "        .orderBy(\"hour\")\n",
    ")\n",
    "\n",
    "hourly_pd = df_hourly.toPandas()\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(hourly_pd[\"hour\"], hourly_pd[\"count\"], color=\"steelblue\")\n",
    "plt.title(\"Tweet Distribution by Hour of Day (2014+)\")\n",
    "plt.xlabel(\"Hour of Day\")\n",
    "plt.ylabel(\"Number of Tweets\")\n",
    "plt.xticks(range(0, 24))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# patterns by day of the week\n",
    "df_dow = (\n",
    "    df_time\n",
    "        .withColumn(\"day_of_week\", date_format(\"publish_ts\", \"E\"))\n",
    "        .groupBy(\"day_of_week\")\n",
    "        .count()\n",
    ")\n",
    "\n",
    "dow_pd = df_dow.toPandas().sort_values(\"day_of_week\")\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(dow_pd[\"day_of_week\"], dow_pd[\"count\"], color=\"darkgreen\")\n",
    "plt.title(\"Tweet Distribution by Day of Week (2014+)\")\n",
    "plt.xlabel(\"Day of Week\")\n",
    "plt.ylabel(\"Number of Tweets\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6b0e30",
   "metadata": {},
   "source": [
    "## Activation and Deactivation Cascades\n",
    "\n",
    "Here we look at **when accounts appear and disappear** in the data:\n",
    "\n",
    "- For each handle, compute the **first** and **last** time it tweeted.\n",
    "- Count how many accounts become active in each time period (e.g. per month or per year).\n",
    "- Similarly, count how many accounts stop tweeting (“deactivate”) in each period.\n",
    "\n",
    "Conceptually, this gives us **activation / deactivation cascades** over time:\n",
    "- Surges in new accounts can signal coordinated onboarding of new personas.\n",
    "- Clusters of deactivation may correspond to clean-up, bans, or strategic shifts.\n",
    "\n",
    "These patterns help us understand how the IRA “portfolio” of accounts was managed over time, beyond just raw tweet volume.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e94453c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    to_timestamp, to_date,\n",
    "    min as spark_min, max as spark_max\n",
    ")\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# timestamp column check \n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "if \"df_time\" not in globals():\n",
    "    df_time = df.withColumn(\n",
    "        \"publish_ts\",\n",
    "        to_timestamp(\"publish_date\", \"M/d/yyyy H:mm\")\n",
    "    )\n",
    "\n",
    "df_time = df_time.filter(df_time.publish_ts.isNotNull())\n",
    "df_time = df_time.filter(df_time.publish_ts >= \"2014-01-01\")   # <-- ADDED LINE\n",
    "\n",
    "# 1st and last tweet / account\n",
    "account_life = (\n",
    "    df_time.groupBy(\"author\")\n",
    "           .agg(\n",
    "               spark_min(\"publish_ts\").alias(\"first_tweet\"),\n",
    "               spark_max(\"publish_ts\").alias(\"last_tweet\")\n",
    "           )\n",
    ")\n",
    "\n",
    "# activation and deactivation counts / day\n",
    "activation_df = (\n",
    "    account_life\n",
    "        .withColumn(\"activation_date\", to_date(\"first_tweet\"))\n",
    "        .groupBy(\"activation_date\")\n",
    "        .count()\n",
    "        .orderBy(\"activation_date\")\n",
    ")\n",
    "\n",
    "deactivation_df = (\n",
    "    account_life\n",
    "        .withColumn(\"deactivation_date\", to_date(\"last_tweet\"))\n",
    "        .groupBy(\"deactivation_date\")\n",
    "        .count()\n",
    "        .orderBy(\"deactivation_date\")\n",
    ")\n",
    "\n",
    "activation_pd = activation_df.toPandas()\n",
    "activation_pd[\"activation_date\"] = pd.to_datetime(activation_pd[\"activation_date\"])\n",
    "activation_pd = activation_pd.sort_values(\"activation_date\")\n",
    "\n",
    "deactivation_pd = deactivation_df.toPandas()\n",
    "deactivation_pd[\"deactivation_date\"] = pd.to_datetime(deactivation_pd[\"deactivation_date\"])\n",
    "deactivation_pd = deactivation_pd.sort_values(\"deactivation_date\")\n",
    "\n",
    "# load events csv\n",
    "events_path = \"/Users/sanduandrei/Desktop/BigData/data/events/events_temporal_markers.csv\"\n",
    "events_pd = pd.read_csv(events_path)\n",
    "events_pd[\"event_date\"] = pd.to_datetime(events_pd[\"date\"], format=\"%m/%d/%Y\")\n",
    "\n",
    "colors = plt.cm.tab20.colors  # color cycle for events\n",
    "\n",
    "# activation cascades plt \n",
    "fig, ax = plt.subplots(figsize=(18, 5))\n",
    "\n",
    "ax.plot(\n",
    "    activation_pd[\"activation_date\"],\n",
    "    activation_pd[\"count\"],\n",
    "    linewidth=1.5,\n",
    "    color=\"black\",\n",
    "    label=\"Account activations\"\n",
    ")\n",
    "\n",
    "for idx, (_, row) in enumerate(events_pd.iterrows()):\n",
    "    ax.axvline(\n",
    "        row[\"event_date\"],\n",
    "        color=colors[idx % len(colors)],\n",
    "        linestyle=\"--\",\n",
    "        linewidth=1.2,\n",
    "        alpha=0.9,\n",
    "        label=row[\"label\"]\n",
    "    )\n",
    "\n",
    "ax.set_title(\"Account Activations Over Time with Key Events\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Number of Accounts First Tweeting\")\n",
    "\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\", fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# deactivation cascades plt \n",
    "fig, ax = plt.subplots(figsize=(18, 5))\n",
    "\n",
    "ax.plot(\n",
    "    deactivation_pd[\"deactivation_date\"],\n",
    "    deactivation_pd[\"count\"],\n",
    "    linewidth=1.5,\n",
    "    color=\"red\",\n",
    "    label=\"Account deactivations\"\n",
    ")\n",
    "\n",
    "for idx, (_, row) in enumerate(events_pd.iterrows()):\n",
    "    ax.axvline(\n",
    "        row[\"event_date\"],\n",
    "        color=colors[idx % len(colors)],\n",
    "        linestyle=\"--\",\n",
    "        linewidth=1.2,\n",
    "        alpha=0.9,\n",
    "        label=row[\"label\"]\n",
    "    )\n",
    "\n",
    "ax.set_title(\"Account Deactivations Over Time with Key Events\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Number of Accounts Last Tweeting\")\n",
    "\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\", fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34866dc",
   "metadata": {},
   "source": [
    "## Investigating activity of accounts belonging to different categories\n",
    "\n",
    "The IRA accounts are annotated with an **`account_category`** (e.g. LeftTroll, RightTroll, NewsFeed, etc.).  \n",
    "In this section we:\n",
    "\n",
    "- Group tweets by `account_category` and time (e.g. monthly counts).\n",
    "- Compare **activity patterns** across categories: which categories dominate at different points in time?\n",
    "- Visualise how the relative share of total tweets per category evolves.\n",
    "\n",
    "The objective is to reveal whether certain categories were:\n",
    "- Persistent over the whole period,\n",
    "- Or instead used in **waves** (e.g. bursts of “news feed” accounts followed by more polarised political trolls).\n",
    "\n",
    "These results are directly useful for slides that contrast **strategic roles** of different account types.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418d01de",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "df_time = df.withColumn(\n",
    "    \"publish_ts\",\n",
    "    to_timestamp(\"publish_date\", \"M/d/yyyy H:mm\")  # handles 1/2/2017 14:39 etc.\n",
    ")\n",
    "\n",
    "# tweets per month by account category \n",
    "df_cat_monthly = (\n",
    "    df_time\n",
    "        .where(col(\"publish_ts\").isNotNull())\n",
    "        .groupBy(\n",
    "            year(\"publish_ts\").alias(\"year\"),\n",
    "            month(\"publish_ts\").alias(\"month\"),\n",
    "            col(\"account_category\")\n",
    "        )\n",
    "        .count()\n",
    ")\n",
    "\n",
    "df_cat_monthly.show(10, truncate=False)\n",
    "\n",
    "# top n categories by tweet volume \n",
    "cat_totals = (\n",
    "    df_cat_monthly\n",
    "        .groupBy(\"account_category\")\n",
    "        .agg(spark_sum(\"count\").alias(\"total_tweets\"))\n",
    "        .orderBy(col(\"total_tweets\").desc())\n",
    ")\n",
    "\n",
    "cat_totals.show(truncate=False)\n",
    "\n",
    "# choose top 4–5 categories for a clean plot\n",
    "TOP_N = 5\n",
    "top_categories = [\n",
    "    row[\"account_category\"]\n",
    "    for row in cat_totals.limit(TOP_N).collect()\n",
    "    if row[\"account_category\"] is not None\n",
    "]\n",
    "\n",
    "print(\"Top categories to plot:\", top_categories)\n",
    "\n",
    "# monthly data convert to pandas \n",
    "cat_monthly_pd = df_cat_monthly.toPandas()\n",
    "\n",
    "# filter to top categories only\n",
    "cat_monthly_pd = cat_monthly_pd[cat_monthly_pd[\"account_category\"].isin(top_categories)]\n",
    "\n",
    "# build a proper year_month datetime for x-axis\n",
    "cat_monthly_pd[\"year_month\"] = pd.to_datetime(\n",
    "    cat_monthly_pd[\"year\"].astype(str) + \"-\" +\n",
    "    cat_monthly_pd[\"month\"].astype(str).str.zfill(2) + \"-01\"\n",
    ")\n",
    "\n",
    "# pivot: rows = year_month, columns = account_category, values = tweet counts\n",
    "pivot = cat_monthly_pd.pivot_table(\n",
    "    index=\"year_month\",\n",
    "    columns=\"account_category\",\n",
    "    values=\"count\",\n",
    "    aggfunc=\"sum\",\n",
    "    fill_value=0\n",
    ").sort_index()\n",
    "\n",
    "pivot.head()\n",
    "\n",
    "# Tweets over time by account_category\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "\n",
    "for category in pivot.columns:\n",
    "    plt.plot(pivot.index, pivot[category], marker=\"o\", linewidth=1, label=category)\n",
    "\n",
    "plt.title(\"Monthly Tweet Volume by Account Category\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Number of Tweets\")\n",
    "plt.legend(title=\"Account Category\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a261cd02",
   "metadata": {},
   "source": [
    "## Tweet distribution statistics\n",
    "- Detecting spread of nunique tweets\n",
    "\n",
    "We now move from aggregate time series to **per-account behaviour**:\n",
    "\n",
    "- Compute the **distribution of tweet counts per account**.\n",
    "- Summarise this distribution with descriptive statistics (mean, median, percentiles).\n",
    "- Visualise it (often on a log scale) to see how skewed the activity is.\n",
    "\n",
    "This tells us whether most accounts tweet a similar amount, or whether there are:\n",
    "- Many low-activity accounts, and\n",
    "- A small number of very prolific accounts.\n",
    "\n",
    "Understanding this distribution is key before performing any **outlier detection** or focusing on “heavy hitters”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c7777a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_per_account_spark = (\n",
    "    df.groupBy(\"author\")\n",
    "      .count()\n",
    "      .withColumnRenamed(\"count\", \"tweets_per_account\")\n",
    ")\n",
    "\n",
    "tweets_per_account = tweets_per_account_spark.toPandas()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(tweets_per_account[\"tweets_per_account\"], bins=50, color=\"steelblue\", edgecolor=\"black\")\n",
    "plt.title(\"Distribution of Tweets per Account (Linear Scale)\")\n",
    "plt.xlabel(\"Number of Tweets\")\n",
    "plt.ylabel(\"Number of Accounts\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(tweets_per_account[\"tweets_per_account\"], bins=50, color=\"salmon\", edgecolor=\"black\", log=True)\n",
    "plt.title(\"Distribution of Tweets per Account (Log Scale)\")\n",
    "plt.xlabel(\"Number of Tweets (log scale)\")\n",
    "plt.ylabel(\"Number of Accounts\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a7992f",
   "metadata": {},
   "source": [
    "## Detecting outliers \n",
    "\n",
    "Based on the tweet count distribution, we next identify **outlier accounts**:\n",
    "\n",
    "- Define a rule to flag unusually active accounts (e.g. top percentile, IQR-based threshold, or a fixed cut-off).\n",
    "- Extract a list of accounts whose tweet volume is far above the bulk of the distribution.\n",
    "- Optionally visualise the threshold on top of the histogram / density plot.\n",
    "\n",
    "These outliers are candidates for:\n",
    "- **Central operational accounts** or hubs in the campaign,\n",
    "- Or automated / scripted behaviour.\n",
    "\n",
    "Later, we will relate these particularly active accounts to both **textual themes** and **network position**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07cad56",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_per_account = (\n",
    "    df.groupBy(\"author\")\n",
    "      .agg(count(\"*\").alias(\"tweets_per_account\"))\n",
    ")\n",
    "\n",
    "# high activity datasets\n",
    "accounts_over_50000 = tweets_per_account.filter(col(\"tweets_per_account\") > 50000)\n",
    "accounts_over_40000 = tweets_per_account.filter(col(\"tweets_per_account\") > 40000)\n",
    "\n",
    "\n",
    "print(\"Accounts with > 50,000 tweets:\")\n",
    "accounts_over_50000.show(truncate=False)\n",
    "\n",
    "print(\"\\nAccounts with > 40,000 tweets:\")\n",
    "accounts_over_40000.show(truncate=False)\n",
    "\n",
    "# convertion to pandas for inspection\n",
    "accounts_50k_pd = accounts_over_50000.toPandas()\n",
    "accounts_40k_pd = accounts_over_40000.toPandas()\n",
    "\n",
    "accounts_50k_pd, accounts_40k_pd\n",
    "\n",
    "# tweets per account\n",
    "tweets_per_account = (\n",
    "    df.groupBy(\"author\")\n",
    "      .agg(count(\"*\").alias(\"tweets_per_account\"))\n",
    ")\n",
    "\n",
    "# pandas for percentile calc\n",
    "tp_pd = tweets_per_account.toPandas()\n",
    "\n",
    "# # 95th percentile \n",
    "p95_threshold = tp_pd[\"tweets_per_account\"].quantile(0.95)\n",
    "\n",
    "print(f\"90th percentile tweet count threshold: {p95_threshold:.2f}\")\n",
    "\n",
    "# nunique accounts above 95th percentile\n",
    "n_above_90 = (tp_pd[\"tweets_per_account\"] > p95_threshold).sum()\n",
    "\n",
    "print(f\"Number of accounts above 95th percentile: {n_above_90}\")\n",
    "\n",
    "# show as df\n",
    "accounts_above_90 = tweets_per_account.filter(col(\"tweets_per_account\") > p95_threshold)\n",
    "accounts_above_90.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f6706f",
   "metadata": {},
   "source": [
    "## Further exploration of the outliers \n",
    "\n",
    "After identifying high-activity accounts, we examine their characteristics in more detail:\n",
    "\n",
    "- Break down outlier accounts by **account_category** to see which categories dominate among heavy hitters.\n",
    "- Inspect basic temporal patterns (e.g. are these accounts active for longer periods? do they have sustained high activity?).\n",
    "- Optionally look at exemplar tweets to qualitatively understand what these accounts are doing.\n",
    "\n",
    "This helps answer questions such as:\n",
    "- Are the most active accounts primarily “news feeds”, “political trolls”, or something else?\n",
    "- Do these accounts behave differently over time compared to the typical account?\n",
    "\n",
    "These insights will feed nicely into slides that highlight a **small core of highly active operators**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3329a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_per_account = (\n",
    "    df.groupBy(\"author\")\n",
    "      .agg(count(\"*\").alias(\"tweets_per_account\"))\n",
    ")\n",
    "\n",
    "tp_pd = tweets_per_account.toPandas()\n",
    "p95_threshold = tp_pd[\"tweets_per_account\"].quantile(0.95)\n",
    "\n",
    "p95_threshold\n",
    "\n",
    "accounts_above_95 = (\n",
    "    tweets_per_account\n",
    "        .filter(col(\"tweets_per_account\") > p95_threshold)\n",
    "        .withColumnRenamed(\"author\", \"account\")\n",
    ")\n",
    "\n",
    "accounts_above_95.show(truncate=False)\n",
    "print(\"Number of accounts above 95th percentile:\", accounts_above_95.count())\n",
    "\n",
    "df_top5 = (\n",
    "    df.join(accounts_above_95, df.author == accounts_above_95.account, \"inner\")\n",
    "      .drop(accounts_above_95.account)\n",
    ")\n",
    "\n",
    "df_top5.printSchema()\n",
    "print(\"Total tweets from top 5% accounts:\", df_top5.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ed9fcf",
   "metadata": {},
   "source": [
    "## Text and Topic Exploration\n",
    "\n",
    "We now turn to the **content of the tweets** and investigate what these accounts talk about:\n",
    "\n",
    "- Clean and normalise tweet text (lowercasing, removing URLs / punctuation, tokenisation).\n",
    "- Filter to **English-language tweets** and a relevant time window (e.g. from 2014 onwards).\n",
    "- Compute basic word frequencies to obtain a first view on dominant terms.\n",
    "\n",
    "The goal is not to build a full topic model at this stage, but to:\n",
    "- Identify **recurring themes and narratives**,\n",
    "- And set up the foundation for more structured analysis of vocabularies and bigrams.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cab8131",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lower, regexp_replace, split, explode, year\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "# restrict to english words and start from 2014 \n",
    "df_text = (\n",
    "    df_time\n",
    "        .filter(col(\"language\") == \"English\")\n",
    "        .filter(df_time.publish_ts >= \"2014-01-01\")\n",
    ")\n",
    "\n",
    "# text cleaning\n",
    "df_clean = (\n",
    "    df_text\n",
    "        .withColumn(\"text_lower\", lower(col(\"content\")))\n",
    "        .fillna({\"text_lower\": \"\"})\n",
    "        # remove URLs\n",
    "        .withColumn(\"text_lower\", regexp_replace(col(\"text_lower\"), r\"http\\S+\", \" \"))\n",
    "        .withColumn(\"text_lower\", regexp_replace(col(\"text_lower\"), r\"www\\.\\S+\", \" \"))\n",
    "        # remove @mentions\n",
    "        .withColumn(\"text_lower\", regexp_replace(col(\"text_lower\"), r\"@\\w+\", \" \"))\n",
    "        # keep letters only; remove hashtags entirely\n",
    "        .withColumn(\"text_lower\", regexp_replace(col(\"text_lower\"), r\"#\\w+\", \" \"))   # <-- REMOVE HASHTAGS\n",
    "        .withColumn(\"text_lower\", regexp_replace(col(\"text_lower\"), r\"[^a-z]\", \" \"))\n",
    "        # collapse whitespace\n",
    "        .withColumn(\"text_lower\", regexp_replace(col(\"text_lower\"), r\"\\s+\", \" \"))\n",
    ")\n",
    "\n",
    "# tokenize\n",
    "df_tokens = df_clean.withColumn(\"token\", explode(split(col(\"text_lower\"), \" \")))\n",
    "\n",
    "# stopword filter\n",
    "stopwords = StopWordsRemover.loadDefaultStopWords(\"english\")\n",
    "extra_stops = [\"rt\", \"amp\", \"https\", \"http\"]\n",
    "stopwords_all = stopwords + extra_stops\n",
    "\n",
    "df_words = (\n",
    "    df_tokens\n",
    "        .filter(col(\"token\") != \"\")\n",
    "        .filter(~col(\"token\").isin(stopwords_all))\n",
    ")\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "top_words_global = (\n",
    "    df_words.groupBy(\"token\")\n",
    "            .count()\n",
    "            .orderBy(desc(\"count\"))\n",
    ")\n",
    "\n",
    "top_words_global.show(50, truncate=False)\n",
    "\n",
    "\n",
    "df_words_year = df_words.withColumn(\"year\", year(col(\"publish_ts\")))\n",
    "df_words_year = df_words_year.filter(col(\"year\").between(2015, 2017))\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "words_per_year = (\n",
    "    df_words_year\n",
    "        .groupBy(\"year\", \"token\")\n",
    "        .count()\n",
    ")\n",
    "\n",
    "w = Window.partitionBy(\"year\").orderBy(desc(\"count\"))\n",
    "\n",
    "top_words_per_year = (\n",
    "    words_per_year\n",
    "        .withColumn(\"rn\", row_number().over(w))\n",
    "        .filter(col(\"rn\") <= 20)\n",
    "        .orderBy(\"year\", \"rn\")\n",
    ")\n",
    "\n",
    "top_words_per_year.show(60, truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db97e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc, year, month, col\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# top 20 words but we only use top 10 later \n",
    "top20 = (\n",
    "    df_words.groupBy(\"token\")\n",
    "            .count()\n",
    "            .orderBy(desc(\"count\"))\n",
    "            .limit(20)\n",
    "            .toPandas()\n",
    ")\n",
    "\n",
    "top20 = top20.sort_values(\"count\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "top10_words = top20[\"token\"].iloc[:5].tolist()      # top 5\n",
    "next10_words = top20[\"token\"].iloc[5:10].tolist()   # ranks 6–10\n",
    "\n",
    "top10_words, next10_words\n",
    "\n",
    "# monthly counts per word\n",
    "df_words_monthly = (\n",
    "    df_words\n",
    "        .withColumn(\"year\", year(col(\"publish_ts\")))\n",
    "        .withColumn(\"month\", month(col(\"publish_ts\")))\n",
    "        .groupBy(\"year\", \"month\", \"token\")\n",
    "        .count()\n",
    ")\n",
    "\n",
    "words_monthly_pd = df_words_monthly.toPandas()\n",
    "words_monthly_pd[\"year_month\"] = pd.to_datetime(\n",
    "    words_monthly_pd[\"year\"].astype(str) + \"-\" +\n",
    "    words_monthly_pd[\"month\"].astype(str).str.zfill(2) + \"-01\"\n",
    ")\n",
    "words_monthly_pd = words_monthly_pd.sort_values(\"year_month\")\n",
    "\n",
    "# monthly total word counts\n",
    "df_monthly_totals = (\n",
    "    df_words\n",
    "        .withColumn(\"year\", year(col(\"publish_ts\")))\n",
    "        .withColumn(\"month\", month(col(\"publish_ts\")))\n",
    "        .groupBy(\"year\", \"month\")\n",
    "        .count()\n",
    "        .withColumnRenamed(\"count\", \"total_words_month\")\n",
    ")\n",
    "\n",
    "monthly_totals_pd = df_monthly_totals.toPandas()\n",
    "monthly_totals_pd[\"year_month\"] = pd.to_datetime(\n",
    "    monthly_totals_pd[\"year\"].astype(str) + \"-\" +\n",
    "    monthly_totals_pd[\"month\"].astype(str).str.zfill(2) + \"-01\"\n",
    ")\n",
    "\n",
    "# compute normalized frequency \n",
    "wm = words_monthly_pd.merge(\n",
    "    monthly_totals_pd[[\"year_month\", \"total_words_month\"]],\n",
    "    on=\"year_month\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "wm[\"norm_freq\"] = wm[\"count\"] / wm[\"total_words_month\"]\n",
    "\n",
    "# load events csv \n",
    "events_path = \"/Users/sanduandrei/Desktop/BigData/data/events/events_temporal_markers.csv\"\n",
    "events_pd = pd.read_csv(events_path)\n",
    "events_pd[\"event_date\"] = pd.to_datetime(events_pd[\"date\"], format=\"%m/%d/%Y\")\n",
    "\n",
    "colors_events = plt.cm.tab20.colors\n",
    "\n",
    "# plt top 5 words normalized\n",
    "fig, ax = plt.subplots(figsize=(18, 6))\n",
    "\n",
    "df_top10 = wm[wm[\"token\"].isin(top10_words)]\n",
    "\n",
    "for word in top10_words:\n",
    "    sub = df_top10[df_top10[\"token\"] == word]\n",
    "    ax.plot(sub[\"year_month\"], sub[\"norm_freq\"], marker=\"o\", linewidth=1.5, label=word)\n",
    "\n",
    "for idx, (_, row) in enumerate(events_pd.iterrows()):\n",
    "    ax.axvline(\n",
    "        row[\"event_date\"],\n",
    "        color=colors_events[idx % len(colors_events)],\n",
    "        linestyle=\"--\",\n",
    "        linewidth=1,\n",
    "        alpha=0.6,\n",
    "        label=row[\"label\"]\n",
    "    )\n",
    "\n",
    "ax.set_title(\"Top 5 Most Frequent Words — Normalized Monthly Frequency with Key Events\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Normalized Frequency\")\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\", fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# plt words 6-10 normalized\n",
    "fig, ax = plt.subplots(figsize=(18, 6))\n",
    "\n",
    "df_next10 = wm[wm[\"token\"].isin(next10_words)]\n",
    "\n",
    "for word in next10_words:\n",
    "    sub = df_next10[df_next10[\"token\"] == word]\n",
    "    ax.plot(sub[\"year_month\"], sub[\"norm_freq\"], marker=\"o\", linewidth=1.5, label=word)\n",
    "\n",
    "for idx, (_, row) in enumerate(events_pd.iterrows()):\n",
    "    ax.axvline(\n",
    "        row[\"event_date\"],\n",
    "        color=colors_events[idx % len(colors_events)],\n",
    "        linestyle=\"--\",\n",
    "        linewidth=1,\n",
    "        alpha=0.6,\n",
    "        label=row[\"label\"]\n",
    "    )\n",
    "\n",
    "ax.set_title(\"Words Ranked 6–10 — Normalized Monthly Frequency with Key Events\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Normalized Frequency\")\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\", fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5083a375",
   "metadata": {},
   "source": [
    "### Category-Specific Vocabularies\n",
    "\n",
    "Using the cleaned tokens, we compare vocabularies across **account categories**:\n",
    "\n",
    "- For each `account_category`, compute the **most frequent words**.\n",
    "- Optionally filter out stopwords and very generic terms to focus on informative tokens.\n",
    "- Present the top words per category in tables or bar charts.\n",
    "\n",
    "This allows us to see how different categories specialise in different narratives, for example:\n",
    "- “News feed” style accounts focusing on neutral news or event descriptors.\n",
    "- Political troll accounts using more **ideologically charged** or **polarising** vocabulary.\n",
    "\n",
    "These contrasts are useful to show in the presentation to illustrate how **messaging was tailored** by role.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652ef9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, desc, count as spark_count, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# category and sizes \n",
    "category_counts = (\n",
    "    df_words\n",
    "        .groupBy(\"account_category\")\n",
    "        .agg(spark_count(\"*\").alias(\"n_tokens\"))\n",
    "        .orderBy(desc(\"n_tokens\"))\n",
    ")\n",
    "\n",
    "category_counts.show(truncate=False)\n",
    "\n",
    "# only top n categories, keeps things readable \n",
    "TOP_CATS = 5\n",
    "top_categories = [\n",
    "    row[\"account_category\"]\n",
    "    for row in category_counts.limit(TOP_CATS).collect()\n",
    "    if row[\"account_category\"] is not None\n",
    "]\n",
    "\n",
    "print(\"Top categories:\", top_categories)\n",
    "\n",
    "df_words_cat = df_words.filter(col(\"account_category\").isin(top_categories))\n",
    "\n",
    "# word frequencies per category \n",
    "words_by_cat = (\n",
    "    df_words_cat\n",
    "        .groupBy(\"account_category\", \"token\")\n",
    "        .agg(spark_count(\"*\").alias(\"count\"))\n",
    ")\n",
    "\n",
    "# rank words by frequency \n",
    "TOP_K = 20\n",
    "\n",
    "w_cat = Window.partitionBy(\"account_category\").orderBy(desc(\"count\"))\n",
    "\n",
    "top_words_by_cat = (\n",
    "    words_by_cat\n",
    "        .withColumn(\"rank\", row_number().over(w_cat))\n",
    "        .filter(col(\"rank\") <= TOP_K)\n",
    "        .orderBy(\"account_category\", \"rank\")\n",
    ")\n",
    "\n",
    "top_words_by_cat.show(200, truncate=False)\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# convert to pandas\n",
    "top_words_pd = top_words_by_cat.toPandas()\n",
    "\n",
    "# pivot so each category is a row, each rank a column\n",
    "table_df = (\n",
    "    top_words_pd\n",
    "        .sort_values([\"account_category\", \"rank\"])\n",
    "        .pivot(index=\"account_category\", columns=\"rank\", values=\"token\")\n",
    ")\n",
    "\n",
    "# column headers: 1, 2, 3, ...\n",
    "table_df.columns = [str(i) for i in table_df.columns]\n",
    "\n",
    "# matplotlib table \n",
    "fig, ax = plt.subplots(figsize=(14, 4 + 0.4 * len(table_df)))\n",
    "ax.axis(\"off\")\n",
    "\n",
    "mpl_table = ax.table(\n",
    "    cellText=table_df.values,\n",
    "    rowLabels=table_df.index,\n",
    "    colLabels=table_df.columns,\n",
    "    cellLoc=\"center\",\n",
    "    loc=\"center\"\n",
    ")\n",
    "\n",
    "mpl_table.auto_set_font_size(False)\n",
    "mpl_table.set_fontsize(10)\n",
    "mpl_table.scale(1.3, 1.4)\n",
    "\n",
    "# make columns a bit wider so words fit better\n",
    "mpl_table.auto_set_column_width(col=list(range(len(table_df.columns) + 1)))\n",
    "\n",
    "# style header row\n",
    "for (row, col), cell in mpl_table.get_celld().items():\n",
    "    if row == 0:  # header row\n",
    "        cell.set_text_props(weight=\"bold\", color=\"white\")\n",
    "        cell.set_facecolor(\"#4E79A7\")\n",
    "\n",
    "# zebra-strip data rows\n",
    "for (row, col), cell in mpl_table.get_celld().items():\n",
    "    if row > 0:  # skip header\n",
    "        if row % 2 == 0:\n",
    "            cell.set_facecolor(\"#F5F5F5\")\n",
    "        else:\n",
    "            cell.set_facecolor(\"white\")\n",
    "\n",
    "plt.title(\"Top Words by Account Category\", fontsize=16, pad=25)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55163b38",
   "metadata": {},
   "source": [
    "## Bigram Analysis (Two-Word Topics)\n",
    "\n",
    "Single words can be ambiguous, so we next look at **bigrams** (two-word sequences) as proxies for simple “topics”:\n",
    "\n",
    "- Construct bigrams from the cleaned token sequences.\n",
    "- Count bigram frequencies over the corpus.\n",
    "- Filter to informative bigrams (e.g. minimum frequency) to remove noise.\n",
    "\n",
    "Common bigrams such as “breaking news”, “gun control”, or “police shooting” (exact examples depend on results) typically correspond to more concrete issues or frames.\n",
    "\n",
    "This analysis provides a more **structured view of narratives** than unigrams alone and is easy to translate into clear slide visuals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279cb713",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, lower, regexp_replace, split, explode,\n",
    "    year, desc\n",
    ")\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "df_text_big = (\n",
    "    df_time\n",
    "        .filter(col(\"language\") == \"English\")\n",
    "        .filter(df_time.publish_ts >= \"2014-01-01\")\n",
    "        .select(\"publish_ts\", \"account_category\", \"content\")\n",
    ")\n",
    "\n",
    "# Clean text (similar to unigram analysis, but keep arrays)\n",
    "#    - lowercase\n",
    "#    - remove URLs, mentions, hashtags, non-letters\n",
    "\n",
    "df_clean_big = (\n",
    "    df_text_big\n",
    "        .withColumn(\"text_lower\", lower(col(\"content\")))\n",
    "        .fillna({\"text_lower\": \"\"})\n",
    "        # remove URLs\n",
    "        .withColumn(\"text_lower\", regexp_replace(col(\"text_lower\"), r\"http\\S+\", \" \"))\n",
    "        .withColumn(\"text_lower\", regexp_replace(col(\"text_lower\"), r\"www\\.\\S+\", \" \"))\n",
    "        # remove @mentions\n",
    "        .withColumn(\"text_lower\", regexp_replace(col(\"text_lower\"), r\"@\\w+\", \" \"))\n",
    "        # remove hashtags entirely\n",
    "        .withColumn(\"text_lower\", regexp_replace(col(\"text_lower\"), r\"#\\w+\", \" \"))\n",
    "        # keep letters only\n",
    "        .withColumn(\"text_lower\", regexp_replace(col(\"text_lower\"), r\"[^a-z]\", \" \"))\n",
    "        # collapse whitespace\n",
    "        .withColumn(\"text_lower\", regexp_replace(col(\"text_lower\"), r\"\\s+\", \" \"))\n",
    ")\n",
    "\n",
    "# tokenize into raw tokens (arrays)\n",
    "df_tokens_big = df_clean_big.withColumn(\"tokens_raw\", split(col(\"text_lower\"), \" \"))\n",
    "\n",
    "# remove stopwords\n",
    "stopwords = set(StopWordsRemover.loadDefaultStopWords(\"english\") +\n",
    "                [\"rt\", \"amp\", \"https\", \"http\", \"\"])\n",
    "\n",
    "@udf(ArrayType(StringType()))\n",
    "def filter_stopwords(tokens):\n",
    "    if tokens is None:\n",
    "        return []\n",
    "    return [t for t in tokens if t not in stopwords]\n",
    "\n",
    "df_tokens_big = df_tokens_big.withColumn(\"tokens_clean\", filter_stopwords(col(\"tokens_raw\")))\n",
    "\n",
    "# build histogram\n",
    "@udf(ArrayType(StringType()))\n",
    "def make_bigrams(tokens):\n",
    "    if tokens is None:\n",
    "        return []\n",
    "    return [tokens[i] + \" \" + tokens[i+1] for i in range(len(tokens) - 1)]\n",
    "\n",
    "df_bigrams = df_tokens_big.withColumn(\"bigrams\", make_bigrams(col(\"tokens_clean\")))\n",
    "\n",
    "# explode to one bigram per row\n",
    "df_bigrams_exploded = (\n",
    "    df_bigrams\n",
    "        .select(\"publish_ts\", \"account_category\", explode(col(\"bigrams\")).alias(\"bigram\"))\n",
    "        .filter(col(\"bigram\") != \"\")\n",
    ")\n",
    "\n",
    "# top bigrams overall\n",
    "top_bigrams_overall = (\n",
    "    df_bigrams_exploded\n",
    "        .groupBy(\"bigram\")\n",
    "        .count()\n",
    "        .orderBy(desc(\"count\"))\n",
    ")\n",
    "\n",
    "print(\"Top 30 bigrams overall:\")\n",
    "top_bigrams_overall.show(30, truncate=False)\n",
    "\n",
    "# top bigrams by year (2015–2017, for example)\n",
    "df_big_year = df_bigrams_exploded.withColumn(\"year\", year(col(\"publish_ts\")))\n",
    "df_big_year = df_big_year.filter(col(\"year\").between(2015, 2017))\n",
    "\n",
    "bigrams_year_counts = (\n",
    "    df_big_year\n",
    "        .groupBy(\"year\", \"bigram\")\n",
    "        .count()\n",
    ")\n",
    "\n",
    "w = Window.partitionBy(\"year\").orderBy(desc(\"count\"))\n",
    "\n",
    "top_bigrams_per_year = (\n",
    "    bigrams_year_counts\n",
    "        .withColumn(\"rank\", row_number().over(w))\n",
    "        .filter(col(\"rank\") <= 15)\n",
    "        .orderBy(\"year\", \"rank\")\n",
    ")\n",
    "\n",
    "print(\"Top 15 bigrams per year (2015–2017):\")\n",
    "top_bigrams_per_year.show(60, truncate=False)\n",
    "\n",
    "# bar chart top 10 bigrams\n",
    "top10_bigrams_pd = top_bigrams_overall.limit(10).toPandas().sort_values(\"count\", ascending=True)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(top10_bigrams_pd[\"bigram\"], top10_bigrams_pd[\"count\"])\n",
    "plt.title(\"Top 10 Bigrams in English IRA Tweets (2014+)\")\n",
    "plt.xlabel(\"Count\")\n",
    "plt.ylabel(\"Bigram\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ab25a0",
   "metadata": {},
   "source": [
    "### Investigating Bigram frequency per year\n",
    "\n",
    "To connect content with time, we study how bigram usage evolves **year by year**:\n",
    "\n",
    "- Recompute bigram counts grouped by **year**.\n",
    "- For selected bigrams (e.g. the top ones overall), track their frequency across years.\n",
    "- Visualise these trajectories to see when specific narratives are emphasised.\n",
    "\n",
    "This helps us identify:\n",
    "- **Issue cycles** (topics that spike around particular years or events),\n",
    "- And whether new narratives were introduced or old ones were phased out.\n",
    "\n",
    "These time-resolved plots are particularly useful for the presentation to show **how messaging adapted over time**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4c0c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# convert to Pandas\n",
    "top_bigram_year_pd = top_bigrams_per_year.toPandas()\n",
    "\n",
    "years = sorted(top_bigram_year_pd[\"year\"].unique())\n",
    "\n",
    "for yr in years:\n",
    "    df_yr = (\n",
    "        top_bigram_year_pd[top_bigram_year_pd[\"year\"] == yr]\n",
    "        .sort_values(\"count\", ascending=True)\n",
    "    )\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(df_yr[\"bigram\"], df_yr[\"count\"], color=\"steelblue\")\n",
    "    plt.title(f\"Top 15 Bigrams in {yr}\")\n",
    "    plt.xlabel(\"Count\")\n",
    "    plt.ylabel(\"Bigram\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c9c8f4",
   "metadata": {},
   "source": [
    "### Top bigrams by category\n",
    "\n",
    "Finally, we combine category information with bigram statistics:\n",
    "\n",
    "- For each `account_category`, compute the **top bigrams**.\n",
    "- Compare which phrases are characteristic for political trolls vs. news-style accounts, etc.\n",
    "- Optionally normalise by the number of tweets per category to avoid favouring the largest groups.\n",
    "\n",
    "This analysis shows **how different account types specialise in different storylines**, for example:\n",
    "- Certain categories might focus more on domestic politics,\n",
    "- Others might emphasise foreign policy, race, or other divisive issues.\n",
    "\n",
    "These results support a narrative in the slides about **segmentation and role-specific messaging** within the IRA ecosystem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d39a1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "bigrams_by_cat = (\n",
    "    df_bigrams_exploded\n",
    "        .groupBy(\"account_category\", \"bigram\")\n",
    "        .count()\n",
    ")\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "w_cat_big = Window.partitionBy(\"account_category\").orderBy(desc(\"count\"))\n",
    "\n",
    "top_bigrams_by_cat = (\n",
    "    bigrams_by_cat\n",
    "        .withColumn(\"rank\", row_number().over(w_cat_big))\n",
    "        .filter(col(\"rank\") <= 10)\n",
    "        .orderBy(\"account_category\", \"rank\")\n",
    ")\n",
    "\n",
    "top_bigrams_by_cat.show(100, truncate=False)\n",
    "\n",
    "\n",
    "bigrams_cat_pd = top_bigrams_by_cat.toPandas()\n",
    "\n",
    "categories = bigrams_cat_pd[\"account_category\"].unique()\n",
    "\n",
    "for cat in categories:\n",
    "    df_cat = (\n",
    "        bigrams_cat_pd[bigrams_cat_pd[\"account_category\"] == cat]\n",
    "        .sort_values(\"count\", ascending=True)\n",
    "    )\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(df_cat[\"bigram\"], df_cat[\"count\"], color=\"darkred\")\n",
    "    plt.title(f\"Top 10 Bigrams — {cat}\")\n",
    "    plt.xlabel(\"Count\")\n",
    "    plt.ylabel(\"Bigram\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1656c4c2",
   "metadata": {},
   "source": [
    "## Network Analysis\n",
    "\n",
    "To conclude our analysis, we investigate the *interaction structure* of IRA accounts through\n",
    "network analysis. While earlier sections explored what accounts talked about and when they\n",
    "were active, network analysis helps us understand **how accounts interacted** and **which\n",
    "accounts played central roles** in the operation.\n",
    "\n",
    "We construct a **directed mention network**:\n",
    "\n",
    "- **Nodes** represent IRA-controlled Twitter accounts.\n",
    "- **Edges** represent one IRA account mentioning another (`@handle`) in a tweet.\n",
    "- **Edge weight** is the number of mentions.\n",
    "\n",
    "This lets us examine:\n",
    "\n",
    "- **Influence hubs**: accounts frequently mentioned by others.  \n",
    "- **Information broadcasters**: accounts that mention many others.  \n",
    "- **Structural leadership**: accounts with high PageRank.\n",
    "\n",
    "PageRank is particularly useful here because it highlights accounts that received attention\n",
    "from *other important accounts* — a hallmark of coordinated influence operations.\n",
    "\n",
    "Understanding the structure of this mention network helps us identify which accounts were\n",
    "central amplifiers of narratives, how different account categories interacted, and which\n",
    "nodes may have acted as “command-and-control” centers within the broader disinformation\n",
    "ecosystem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5445bfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, lower, regexp_replace, split, explode,\n",
    "    count, lit, desc\n",
    ")\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# start from 2014 & english\n",
    "df_net = (\n",
    "    df_time\n",
    "        .filter(col(\"language\") == \"English\")\n",
    "        .filter(df_time.publish_ts >= \"2014-01-01\")\n",
    "        .select(\"author\", \"account_category\", \"content\")\n",
    ")\n",
    "\n",
    "# normalise author handle\n",
    "df_net = df_net.withColumn(\"author_norm\", lower(col(\"author\")))\n",
    "\n",
    "# extract @mentions correctly\n",
    "df_mentions = (\n",
    "    df_net\n",
    "        # keep only @ and word characters for mention detection\n",
    "        .withColumn(\"clean_text\", regexp_replace(col(\"content\"), r\"[^@\\w]\", \" \"))\n",
    "        .withColumn(\"token\", explode(split(col(\"clean_text\"), \" \")))\n",
    "        .filter(col(\"token\").rlike(r\"^@\\w+\"))      # <-- FIXED REGEX\n",
    "        .withColumn(\"src\", col(\"author_norm\"))\n",
    "        .withColumn(\"dst\", lower(regexp_replace(col(\"token\"), \"@\", \"\")))\n",
    "        .select(\"src\", \"dst\")\n",
    "        .filter(col(\"src\") != col(\"dst\"))\n",
    ")\n",
    "\n",
    "print(\"Raw mention rows:\", df_mentions.count())\n",
    "\n",
    "# weighted edges \n",
    "edges_df = (\n",
    "    df_mentions\n",
    "        .groupBy(\"src\", \"dst\")\n",
    "        .agg(count(lit(1)).alias(\"weight\"))\n",
    "        .orderBy(desc(\"weight\"))\n",
    ")\n",
    "\n",
    "edge_count = edges_df.count()\n",
    "print(\"Number of edges in mention graph:\", edge_count)\n",
    "\n",
    "if edge_count == 0:\n",
    "    print(\"No edges detected – check mention extraction / filters.\")\n",
    "else:\n",
    "    edges_pd = edges_df.toPandas()\n",
    "\n",
    "    # networkx directed graph\n",
    "    G = nx.DiGraph()\n",
    "    for _, row in edges_pd.iterrows():\n",
    "        G.add_edge(row[\"src\"], row[\"dst\"], weight=row[\"weight\"])\n",
    "\n",
    "    print(\"Graph nodes:\", G.number_of_nodes(), \"edges:\", G.number_of_edges())\n",
    "\n",
    "    # running PageRank\n",
    "    pagerank_scores = nx.pagerank(G, weight=\"weight\")\n",
    "\n",
    "    pr_df = pd.DataFrame(list(pagerank_scores.items()), columns=[\"handle\", \"pagerank\"])\n",
    "\n",
    "    # join back \n",
    "    authors_pd = (\n",
    "        df_net\n",
    "        .select(\"author_norm\", \"account_category\")\n",
    "        .distinct()\n",
    "        .toPandas()\n",
    "        .rename(columns={\"author_norm\": \"handle\"})\n",
    "    )\n",
    "\n",
    "    pr_df = pr_df.merge(authors_pd, on=\"handle\", how=\"left\")\n",
    "\n",
    "    # sort by PageRank\n",
    "    pr_df = pr_df.sort_values(\"pagerank\", ascending=False)\n",
    "\n",
    "    # look at top 20\n",
    "    display(pr_df.head(20))\n",
    "\n",
    "    # visualise top page rank accounts\n",
    "    top_pr = pr_df.head(15)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(top_pr[\"handle\"], top_pr[\"pagerank\"], color=\"darkblue\")\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.xlabel(\"PageRank Score\")\n",
    "    plt.ylabel(\"Account Handle\")\n",
    "    plt.title(\"Top Accounts by PageRank in Mention Network\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# pick the top PageRank account\n",
    "top_handle = pr_df.iloc[0][\"handle\"]\n",
    "top_handle\n",
    "\n",
    "# build ego network around top node\n",
    "ego_nodes = nx.ego_graph(G, top_handle, radius=1)  # 1-hop neighbourhood\n",
    "\n",
    "# build a small DataFrame for ego nodes with category & PR\n",
    "ego_pr = pr_df[pr_df[\"handle\"].isin(ego_nodes.nodes())].copy()\n",
    "\n",
    "# map category and pagerank into dicts for quick lookup\n",
    "cat_map = dict(zip(ego_pr[\"handle\"], ego_pr[\"account_category\"]))\n",
    "pr_map = dict(zip(ego_pr[\"handle\"], ego_pr[\"pagerank\"]))\n",
    "\n",
    "# colors by category (simple map)\n",
    "unique_cats = ego_pr[\"account_category\"].astype(str).unique()\n",
    "color_map = {cat: i for i, cat in enumerate(unique_cats)}\n",
    "\n",
    "node_colors = [color_map.get(cat_map.get(n, \"Other\"), 0) for n in ego_nodes.nodes()]\n",
    "node_sizes = [3000 * pr_map.get(n, 0.0001) for n in ego_nodes.nodes()]  # scale PR\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "pos = nx.spring_layout(ego_nodes, k=0.5, seed=42)\n",
    "\n",
    "nx.draw_networkx_nodes(ego_nodes, pos,\n",
    "                       node_size=node_sizes,\n",
    "                       node_color=node_colors,\n",
    "                       cmap=plt.cm.tab10,\n",
    "                       alpha=0.9)\n",
    "nx.draw_networkx_edges(ego_nodes, pos, alpha=0.3)\n",
    "nx.draw_networkx_labels(ego_nodes, pos,\n",
    "                        font_size=8)\n",
    "\n",
    "plt.title(f\"Ego Network of Top PageRank Account: {top_handle}\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ego network viz\n",
    "# top 5 accounts by PageRank\n",
    "pr_ira = (\n",
    "    pr_df[pr_df[\"account_category\"].notna()]\n",
    "    .sort_values(\"pagerank\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "top_hubs = pr_ira[\"handle\"].head(5).tolist()\n",
    "print(\"Top IRA hubs:\", top_hubs)\n",
    "\n",
    "# collect neighbours \n",
    "neighbor_candidates = set()\n",
    "\n",
    "for h in top_hubs:\n",
    "    preds = set(G.predecessors(h))\n",
    "    succs = set(G.successors(h))\n",
    "    neighbor_candidates |= preds | succs\n",
    "\n",
    "# remove hubs themselves from candidate pool\n",
    "neighbor_candidates -= set(top_hubs)\n",
    "\n",
    "# rank neighbours by PageRank and keep only the strongest ones\n",
    "neighbors_pr = (\n",
    "    pr_df[pr_df[\"handle\"].isin(neighbor_candidates)]\n",
    "    .sort_values(\"pagerank\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "TOP_NEIGHBORS = 40   # you can tweak this\n",
    "top_neighbors = neighbors_pr[\"handle\"].head(TOP_NEIGHBORS).tolist()\n",
    "\n",
    "nodes_to_keep = set(top_hubs) | set(top_neighbors)\n",
    "G_sub = G.subgraph(nodes_to_keep).copy()\n",
    "\n",
    "print(\"Subgraph nodes:\", len(G_sub.nodes()), \"edges:\", len(G_sub.edges()))\n",
    "\n",
    "# 3. Build metadata for subgraph nodes\n",
    "sub_pr = pr_df[pr_df[\"handle\"].isin(G_sub.nodes())].copy()\n",
    "cat_map = dict(zip(sub_pr[\"handle\"], sub_pr[\"account_category\"]))\n",
    "pr_map  = dict(zip(sub_pr[\"handle\"], sub_pr[\"pagerank\"]))\n",
    "\n",
    "# color: red for top hubs, grey for others\n",
    "node_colors = [\n",
    "    \"tab:red\" if n in top_hubs else \"tab:gray\"\n",
    "    for n in G_sub.nodes()\n",
    "]\n",
    "\n",
    "# size: larger for hubs, smaller for neighbours\n",
    "node_sizes = [\n",
    "    2500 if n in top_hubs else 800\n",
    "    for n in G_sub.nodes()\n",
    "]\n",
    "\n",
    "# label only the hubs to keep the plot readable\n",
    "labels = {n: n for n in top_hubs}\n",
    "\n",
    "plt.figure(figsize=(9, 9))\n",
    "pos = nx.spring_layout(G_sub, k=0.8, seed=42)\n",
    "\n",
    "nx.draw_networkx_nodes(\n",
    "    G_sub, pos,\n",
    "    node_size=node_sizes,\n",
    "    node_color=node_colors,\n",
    "    alpha=0.9\n",
    ")\n",
    "nx.draw_networkx_edges(G_sub, pos, alpha=0.3, arrowsize=10)\n",
    "nx.draw_networkx_labels(G_sub, pos, labels=labels, font_size=9, font_weight=\"bold\")\n",
    "\n",
    "plt.title(\"Top 5 IRA PageRank Accounts and Their Local Mention Network\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fa91c3",
   "metadata": {},
   "source": [
    "### Community structure and influential accounts\n",
    "\n",
    "After constructing the mention network, we go one step further and look at:\n",
    "\n",
    "- **Communities** detected via a modularity-based algorithm (e.g. Louvain).\n",
    "- The **size distribution** of communities to see whether a few large clusters dominate.\n",
    "- The **top accounts by PageRank or in-degree** within the largest communities.\n",
    "\n",
    "These results help us:\n",
    "\n",
    "- Identify groups of accounts that interact heavily with each other (potential coordinated clusters).\n",
    "- Highlight a small set of **key players** that sit at the centre of information flows.\n",
    "\n",
    "This provides good material for slides summarising “who talks to whom” and which handles are structurally central in the IRA ecosystem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c9f027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install python-louvain\n",
    "import community.community_louvain as community_louvain\n",
    "\n",
    "G_undirected = G.to_undirected()\n",
    "\n",
    "partition = community_louvain.best_partition(G_undirected, weight=\"weight\")\n",
    "\n",
    "\n",
    "# build df of communities\n",
    "comm_df = pd.DataFrame(\n",
    "    {\"handle\": list(partition.keys()),\n",
    "     \"community\": list(partition.values())}\n",
    ")\n",
    "\n",
    "# merge with PageRank + category info\n",
    "pr_comm_df = (\n",
    "    pr_df.merge(comm_df, on=\"handle\", how=\"inner\")\n",
    "         .copy()\n",
    ")\n",
    "\n",
    "# community sizes \n",
    "comm_sizes = (\n",
    "    pr_comm_df.groupby(\"community\")[\"handle\"]\n",
    "              .nunique()\n",
    "              .reset_index(name=\"n_nodes\")\n",
    "              .sort_values(\"n_nodes\", ascending=False)\n",
    ")\n",
    "\n",
    "print(\"Top 10 communities by size:\")\n",
    "display(comm_sizes.head(10))\n",
    "\n",
    "# look at PageRank within biggest communities\n",
    "top_communities = comm_sizes.head(3)[\"community\"].tolist()  # adjust number if you want more\n",
    "\n",
    "for c in top_communities:\n",
    "    print(f\"\\n=== Community {c} (size={int(comm_sizes[comm_sizes['community'] == c]['n_nodes'])}) ===\")\n",
    "    sub = (\n",
    "        pr_comm_df[pr_comm_df[\"community\"] == c]\n",
    "        .sort_values(\"pagerank\", ascending=False)\n",
    "        .head(10)\n",
    "    )\n",
    "    display(sub[[\"handle\", \"account_category\", \"pagerank\", \"community\"]])\n",
    "\n",
    "# simple bar chart / cmmunity sizes \n",
    "top_comm_plot = comm_sizes.head(10)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(top_comm_plot[\"community\"].astype(str), top_comm_plot[\"n_nodes\"], color=\"teal\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xlabel(\"Number of Accounts\")\n",
    "plt.ylabel(\"Community ID\")\n",
    "plt.title(\"Largest Louvain Communities in the Mention Network\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
