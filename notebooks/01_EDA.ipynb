{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bd3fa3a",
   "metadata": {},
   "source": [
    "## Importing all dependencies for the Notebook\n",
    "Important for running the notebook. Not necessarilly relevant to our analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe686ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import findspark\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, count, min as spark_min, max as spark_max,\n",
    "    avg, length, to_timestamp, expr, to_date, sum as spark_sum, year, month\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "import time \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee899f3",
   "metadata": {},
   "source": [
    "## PySpark setup inside of notebook. \n",
    "- Notebook initialization in the next two cells\n",
    "- Swap JAVA_HOME path into local path. Check for nano zsh.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afa1143",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Python executable:\", sys.executable)\n",
    "print(\"Python version   :\", sys.version)\n",
    "print(\"JAVA_HOME before :\", os.environ.get(\"JAVA_HOME\"))\n",
    "\n",
    "JAVA_HOME = \"/opt/homebrew/Cellar/openjdk@11/11.0.29/libexec/openjdk.jdk/Contents/Home\" # andrei \n",
    "# JAVA_HOME=\"/Library/Java/JavaVirtualMachines/temurin-11.jdk/Contents/Home\" # ezgim\n",
    "os.environ[\"JAVA_HOME\"] = JAVA_HOME\n",
    "os.environ[\"PATH\"] = os.path.join(JAVA_HOME, \"bin\") + \":\" + os.environ[\"PATH\"]\n",
    "\n",
    "print(\"JAVA_HOME after  :\", os.environ.get(\"JAVA_HOME\"))\n",
    "\n",
    "import subprocess\n",
    "print(\"\\njava -version from this kernel:\")\n",
    "print(subprocess.check_output([\"java\", \"-version\"], stderr=subprocess.STDOUT).decode())\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .appName(\"test_jvm\")\n",
    "        .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"\\nSpark version:\", spark.version)\n",
    "\n",
    "# Tiny test job\n",
    "spark.range(5).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7860e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    project_root = Path(\n",
    "        subprocess.check_output([\"git\", \"rev-parse\", \"--show-toplevel\"])\n",
    "        .decode()\n",
    "        .strip()\n",
    "    )\n",
    "except Exception:\n",
    "    project_root = Path.cwd().parent\n",
    "\n",
    "print(\"PROJECT ROOT:\", project_root)\n",
    "\n",
    "raw_dir = project_root / \"data\" / \"raw\"\n",
    "print(\"RAW DATA DIR:\", raw_dir)\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .appName(\"IRA_Tweets_Task1\")\n",
    "        .master(\"local[*]\")\n",
    "        .config(\"spark.driver.memory\", \"6g\")      # adjust down/up depending on your RAM\n",
    "        .config(\"spark.executor.memory\", \"6g\")    # local = same as driver\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"4\")\n",
    "        .getOrCreate()\n",
    ")\n",
    "\n",
    "df = (\n",
    "    spark.read\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .option(\"multiLine\", \"true\")\n",
    "        .option(\"escape\", \"\\\"\")\n",
    "        .csv(str(raw_dir))\n",
    ")\n",
    "\n",
    "df.printSchema()\n",
    "print(\"Total rows:\", df.count())\n",
    "\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff66e12",
   "metadata": {},
   "source": [
    "## Exploratory analysis \n",
    "We begin by quantifying the dataaset, provding a foundation for understanding how large the operation was and how intensively each account was used. \n",
    "\n",
    "- Exploring the dataset by looking for:\n",
    "    - Total number of tweets\n",
    "    - Total number of unique accounts\n",
    "    - Average tweets per account\n",
    "    - Media tweets per account\n",
    "    - Max tweets by a single amount\n",
    "    - Std dev of tweets per account\n",
    "    - Mean account lifespan (days)\n",
    "    - Average tweet length (characters)\n",
    "    - Average followers per account \n",
    "\n",
    "These statistics show the scale and shape of the operations and help us detect assymetries. The following table summarizes the main dataset priorities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843b9c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# timestamp column\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "df_time = df.withColumn(\n",
    "    \"publish_ts\",\n",
    "    to_timestamp(\"publish_date\", \"M/d/yyyy H:mm\")\n",
    ")\n",
    "\n",
    "# global accounts\n",
    "total_tweets = df.count()\n",
    "n_accounts = df.selectExpr(\"count(distinct author) as n\").first()[\"n\"]\n",
    "\n",
    "# / account stats\n",
    "acct_stats_spark = (\n",
    "    df_time.groupBy(\"author\")\n",
    "           .agg(\n",
    "               count(\"*\").alias(\"n_tweets\"),\n",
    "               spark_min(\"publish_ts\").alias(\"first_ts\"),\n",
    "               spark_max(\"publish_ts\").alias(\"last_ts\"),\n",
    "               avg(\"followers\").alias(\"avg_followers\")\n",
    "           )\n",
    ")\n",
    "\n",
    "acct_stats = acct_stats_spark.toPandas()\n",
    "\n",
    "# lifespan\n",
    "acct_stats[\"lifespan_days\"] = (\n",
    "    (acct_stats[\"last_ts\"] - acct_stats[\"first_ts\"])\n",
    "    .dt.total_seconds() / 86400.0\n",
    ").clip(lower=0)\n",
    "\n",
    "# acc level metrics\n",
    "avg_tweets = acct_stats[\"n_tweets\"].mean()\n",
    "median_tweets = acct_stats[\"n_tweets\"].median()\n",
    "max_tweets = acct_stats[\"n_tweets\"].max()\n",
    "std_tweets = acct_stats[\"n_tweets\"].std()\n",
    "\n",
    "mean_lifespan = acct_stats[\"lifespan_days\"].mean()\n",
    "avg_followers_per_account = acct_stats[\"avg_followers\"].mean()\n",
    "\n",
    "# avg tweet length \n",
    "avg_tweet_length = (\n",
    "    df.select(avg(length(\"content\")).alias(\"avg_len\"))\n",
    "      .first()[\"avg_len\"]\n",
    ")\n",
    "\n",
    "# defining stats table \n",
    "stats_df = pd.DataFrame({\n",
    "    \"Metric\": [\n",
    "        \"Total number of tweets\",\n",
    "        \"Total number of unique accounts\",\n",
    "        \"Average tweets per account\",\n",
    "        \"Median tweets per account\",\n",
    "        \"Max tweets by a single account\",\n",
    "        \"Std dev of tweets per account\",\n",
    "        \"Mean account lifespan (days)\",\n",
    "        \"Average tweet length (characters)\",\n",
    "        \"Average followers per account\",\n",
    "    ],\n",
    "    \"Value\": [\n",
    "        f\"{total_tweets:,}\",\n",
    "        f\"{n_accounts:,}\",\n",
    "        f\"{avg_tweets:.2f}\",\n",
    "        f\"{median_tweets:.2f}\",\n",
    "        f\"{max_tweets:,}\",\n",
    "        f\"{std_tweets:.2f}\",\n",
    "        f\"{mean_lifespan:.2f}\",\n",
    "        f\"{avg_tweet_length:.2f}\",\n",
    "        f\"{avg_followers_per_account:.2f}\",\n",
    "    ]\n",
    "})\n",
    "\n",
    "plt.close('all')\n",
    "\n",
    "# plt \n",
    "fig, ax = plt.subplots(figsize=(9, 3))\n",
    "ax.axis(\"off\")\n",
    "\n",
    "# add title above table\n",
    "fig, ax = plt.subplots(figsize=(9, 3))\n",
    "\n",
    "# move table lower\n",
    "ax.set_position([0, -0.05, 1, 1])\n",
    "ax.axis(\"off\")\n",
    "\n",
    "plt.title(\"Key Dataset Statistics\", pad=20)\n",
    "\n",
    "table = ax.table(\n",
    "    cellText=stats_df.values,\n",
    "    colLabels=stats_df.columns,\n",
    "    loc=\"center\",\n",
    "    cellLoc=\"center\"\n",
    ")\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(11)\n",
    "table.scale(1, 2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e976e0e",
   "metadata": {},
   "source": [
    "## Temporal Analysis: \n",
    "The temporal analysis reveals the heartbeat of the operation. Plotting tweet activity over time allows us to see major surges in activity aligned with political events and periods of heightened coordination. \n",
    "\n",
    "To investigate this narrative, we overlay key geopolitical and political events using dotted vertical lines. These annotation allow us to interpret volume spikes in context.\n",
    "\n",
    "This section includes:\n",
    "- Daily Tweet volume over time \n",
    "- 30 day rolling average of Tweet counts\n",
    "- Posting patterns\n",
    "    - Hour of day \n",
    "    - Day of the week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bbfba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp, to_date, hour, date_format\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# timestamps\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "df_time = df.withColumn(\n",
    "    \"publish_ts\",\n",
    "    to_timestamp(\"publish_date\", \"M/d/yyyy H:mm\")\n",
    ")\n",
    "\n",
    "# keep only tweets from 2014 onwards\n",
    "df_time = (\n",
    "    df_time\n",
    "        .filter(df_time.publish_ts.isNotNull())\n",
    "        .filter(df_time.publish_ts >= \"2014-01-01\")\n",
    ")\n",
    "\n",
    "# daily tweet counts\n",
    "df_daily = (\n",
    "    df_time\n",
    "        .withColumn(\"date\", to_date(\"publish_ts\"))\n",
    "        .groupBy(\"date\")\n",
    "        .count()\n",
    "        .orderBy(\"date\")\n",
    ")\n",
    "\n",
    "daily_pd = df_daily.toPandas()\n",
    "daily_pd[\"date\"] = pd.to_datetime(daily_pd[\"date\"])\n",
    "daily_pd = daily_pd.sort_values(\"date\")\n",
    "\n",
    "# 30-day rolling avg for second plot\n",
    "daily_pd[\"rolling_30\"] = daily_pd[\"count\"].rolling(30, center=True).mean()\n",
    "\n",
    "# load events csv\n",
    "events_path = \"/Users/sanduandrei/Desktop/BigData/data/events/events_temporal_markers.csv\"\n",
    "events_pd = pd.read_csv(events_path)\n",
    "events_pd[\"event_date\"] = pd.to_datetime(events_pd[\"date\"], format=\"%m/%d/%Y\")\n",
    "\n",
    "# daily plot for tweet count\n",
    "fig, ax = plt.subplots(figsize=(18, 6))\n",
    "ax.plot(daily_pd[\"date\"], daily_pd[\"count\"], linewidth=1.5, color=\"black\", label=\"Daily tweets\")\n",
    "\n",
    "colors = plt.cm.tab20.colors\n",
    "for idx, (_, row) in enumerate(events_pd.iterrows()):\n",
    "    ax.axvline(\n",
    "        row[\"event_date\"],\n",
    "        color=colors[idx % len(colors)],\n",
    "        linestyle=\"--\",\n",
    "        linewidth=1.5,\n",
    "        alpha=0.9,\n",
    "        label=row[\"label\"]\n",
    "    )\n",
    "\n",
    "ax.set_title(\"Daily Tweet Volume Over Time with Key Events Marked (2014+)\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Number of Tweets\")\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\", fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 30 day rolling average \n",
    "fig, ax = plt.subplots(figsize=(18, 6))\n",
    "ax.plot(daily_pd[\"date\"], daily_pd[\"rolling_30\"], linewidth=2, color=\"darkred\", label=\"30-day Rolling Avg\")\n",
    "\n",
    "for idx, (_, row) in enumerate(events_pd.iterrows()):\n",
    "    ax.axvline(\n",
    "        row[\"event_date\"],\n",
    "        color=colors[idx % len(colors)],\n",
    "        linestyle=\"--\",\n",
    "        linewidth=1.5,\n",
    "        alpha=0.9,\n",
    "        label=row[\"label\"]\n",
    "    )\n",
    "\n",
    "ax.set_title(\"30-Day Rolling Average of Tweet Volume with Key Events Marked (2014+)\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Tweets (Smoothed Trend)\")\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\", fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# patterns by hour of the day\n",
    "df_hourly = (\n",
    "    df_time\n",
    "        .withColumn(\"hour\", hour(\"publish_ts\"))\n",
    "        .groupBy(\"hour\")\n",
    "        .count()\n",
    "        .orderBy(\"hour\")\n",
    ")\n",
    "\n",
    "hourly_pd = df_hourly.toPandas()\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(hourly_pd[\"hour\"], hourly_pd[\"count\"], color=\"steelblue\")\n",
    "plt.title(\"Tweet Distribution by Hour of Day (2014+)\")\n",
    "plt.xlabel(\"Hour of Day\")\n",
    "plt.ylabel(\"Number of Tweets\")\n",
    "plt.xticks(range(0, 24))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# patterns by day of the week\n",
    "df_dow = (\n",
    "    df_time\n",
    "        .withColumn(\"day_of_week\", date_format(\"publish_ts\", \"E\"))\n",
    "        .groupBy(\"day_of_week\")\n",
    "        .count()\n",
    ")\n",
    "\n",
    "dow_pd = df_dow.toPandas().sort_values(\"day_of_week\")\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(dow_pd[\"day_of_week\"], dow_pd[\"count\"], color=\"darkgreen\")\n",
    "plt.title(\"Tweet Distribution by Day of Week (2014+)\")\n",
    "plt.xlabel(\"Day of Week\")\n",
    "plt.ylabel(\"Number of Tweets\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6b0e30",
   "metadata": {},
   "source": [
    "## Activation and Deactivation Cascades\n",
    "One important aspect to check when looking at coordinated influence campaigns is the simultaneous activation or retirement of many accounts. \n",
    "By examining the first and last tweet timestamps for each account, we can observe waves of activation and deactivation of accounts. \n",
    "\n",
    "- These cascades often occur:\n",
    "    - before major political events\n",
    "    - during narrative pivots\n",
    "    - or after account exposure \n",
    "\n",
    "Detecting these waves can help us identify the operational cycles of the troll factory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e94453c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    to_timestamp, to_date,\n",
    "    min as spark_min, max as spark_max\n",
    ")\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# timestamp column check \n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "if \"df_time\" not in globals():\n",
    "    df_time = df.withColumn(\n",
    "        \"publish_ts\",\n",
    "        to_timestamp(\"publish_date\", \"M/d/yyyy H:mm\")\n",
    "    )\n",
    "\n",
    "df_time = df_time.filter(df_time.publish_ts.isNotNull())\n",
    "df_time = df_time.filter(df_time.publish_ts >= \"2014-01-01\")   # <-- ADDED LINE\n",
    "\n",
    "# 1st and last tweet / account\n",
    "account_life = (\n",
    "    df_time.groupBy(\"author\")\n",
    "           .agg(\n",
    "               spark_min(\"publish_ts\").alias(\"first_tweet\"),\n",
    "               spark_max(\"publish_ts\").alias(\"last_tweet\")\n",
    "           )\n",
    ")\n",
    "\n",
    "# activation and deactivation counts / day\n",
    "activation_df = (\n",
    "    account_life\n",
    "        .withColumn(\"activation_date\", to_date(\"first_tweet\"))\n",
    "        .groupBy(\"activation_date\")\n",
    "        .count()\n",
    "        .orderBy(\"activation_date\")\n",
    ")\n",
    "\n",
    "deactivation_df = (\n",
    "    account_life\n",
    "        .withColumn(\"deactivation_date\", to_date(\"last_tweet\"))\n",
    "        .groupBy(\"deactivation_date\")\n",
    "        .count()\n",
    "        .orderBy(\"deactivation_date\")\n",
    ")\n",
    "\n",
    "activation_pd = activation_df.toPandas()\n",
    "activation_pd[\"activation_date\"] = pd.to_datetime(activation_pd[\"activation_date\"])\n",
    "activation_pd = activation_pd.sort_values(\"activation_date\")\n",
    "\n",
    "deactivation_pd = deactivation_df.toPandas()\n",
    "deactivation_pd[\"deactivation_date\"] = pd.to_datetime(deactivation_pd[\"deactivation_date\"])\n",
    "deactivation_pd = deactivation_pd.sort_values(\"deactivation_date\")\n",
    "\n",
    "# load events csv\n",
    "events_path = \"/Users/sanduandrei/Desktop/BigData/data/events/events_temporal_markers.csv\"\n",
    "events_pd = pd.read_csv(events_path)\n",
    "events_pd[\"event_date\"] = pd.to_datetime(events_pd[\"date\"], format=\"%m/%d/%Y\")\n",
    "\n",
    "colors = plt.cm.tab20.colors  # color cycle for events\n",
    "\n",
    "# activation cascades plt \n",
    "fig, ax = plt.subplots(figsize=(18, 5))\n",
    "\n",
    "ax.plot(\n",
    "    activation_pd[\"activation_date\"],\n",
    "    activation_pd[\"count\"],\n",
    "    linewidth=1.5,\n",
    "    color=\"black\",\n",
    "    label=\"Account activations\"\n",
    ")\n",
    "\n",
    "for idx, (_, row) in enumerate(events_pd.iterrows()):\n",
    "    ax.axvline(\n",
    "        row[\"event_date\"],\n",
    "        color=colors[idx % len(colors)],\n",
    "        linestyle=\"--\",\n",
    "        linewidth=1.2,\n",
    "        alpha=0.9,\n",
    "        label=row[\"label\"]\n",
    "    )\n",
    "\n",
    "ax.set_title(\"Account Activations Over Time with Key Events\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Number of Accounts First Tweeting\")\n",
    "\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\", fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# deactivation cascades plt \n",
    "fig, ax = plt.subplots(figsize=(18, 5))\n",
    "\n",
    "ax.plot(\n",
    "    deactivation_pd[\"deactivation_date\"],\n",
    "    deactivation_pd[\"count\"],\n",
    "    linewidth=1.5,\n",
    "    color=\"red\",\n",
    "    label=\"Account deactivations\"\n",
    ")\n",
    "\n",
    "for idx, (_, row) in enumerate(events_pd.iterrows()):\n",
    "    ax.axvline(\n",
    "        row[\"event_date\"],\n",
    "        color=colors[idx % len(colors)],\n",
    "        linestyle=\"--\",\n",
    "        linewidth=1.2,\n",
    "        alpha=0.9,\n",
    "        label=row[\"label\"]\n",
    "    )\n",
    "\n",
    "ax.set_title(\"Account Deactivations Over Time with Key Events\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Number of Accounts Last Tweeting\")\n",
    "\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\", fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34866dc",
   "metadata": {},
   "source": [
    "## Investigating activity of accounts belonging to different categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418d01de",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "df_time = df.withColumn(\n",
    "    \"publish_ts\",\n",
    "    to_timestamp(\"publish_date\", \"M/d/yyyy H:mm\")  # handles 1/2/2017 14:39 etc.\n",
    ")\n",
    "\n",
    "# tweets per month by account category \n",
    "df_cat_monthly = (\n",
    "    df_time\n",
    "        .where(col(\"publish_ts\").isNotNull())\n",
    "        .groupBy(\n",
    "            year(\"publish_ts\").alias(\"year\"),\n",
    "            month(\"publish_ts\").alias(\"month\"),\n",
    "            col(\"account_category\")\n",
    "        )\n",
    "        .count()\n",
    ")\n",
    "\n",
    "df_cat_monthly.show(10, truncate=False)\n",
    "\n",
    "# top n categories by tweet volume \n",
    "cat_totals = (\n",
    "    df_cat_monthly\n",
    "        .groupBy(\"account_category\")\n",
    "        .agg(spark_sum(\"count\").alias(\"total_tweets\"))\n",
    "        .orderBy(col(\"total_tweets\").desc())\n",
    ")\n",
    "\n",
    "cat_totals.show(truncate=False)\n",
    "\n",
    "# choose top 4–5 categories for a clean plot\n",
    "TOP_N = 5\n",
    "top_categories = [\n",
    "    row[\"account_category\"]\n",
    "    for row in cat_totals.limit(TOP_N).collect()\n",
    "    if row[\"account_category\"] is not None\n",
    "]\n",
    "\n",
    "print(\"Top categories to plot:\", top_categories)\n",
    "\n",
    "# monthly data convert to pandas \n",
    "cat_monthly_pd = df_cat_monthly.toPandas()\n",
    "\n",
    "# filter to top categories only\n",
    "cat_monthly_pd = cat_monthly_pd[cat_monthly_pd[\"account_category\"].isin(top_categories)]\n",
    "\n",
    "# build a proper year_month datetime for x-axis\n",
    "cat_monthly_pd[\"year_month\"] = pd.to_datetime(\n",
    "    cat_monthly_pd[\"year\"].astype(str) + \"-\" +\n",
    "    cat_monthly_pd[\"month\"].astype(str).str.zfill(2) + \"-01\"\n",
    ")\n",
    "\n",
    "# pivot: rows = year_month, columns = account_category, values = tweet counts\n",
    "pivot = cat_monthly_pd.pivot_table(\n",
    "    index=\"year_month\",\n",
    "    columns=\"account_category\",\n",
    "    values=\"count\",\n",
    "    aggfunc=\"sum\",\n",
    "    fill_value=0\n",
    ").sort_index()\n",
    "\n",
    "pivot.head()\n",
    "\n",
    "# Tweets over time by account_category\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "\n",
    "for category in pivot.columns:\n",
    "    plt.plot(pivot.index, pivot[category], marker=\"o\", linewidth=1, label=category)\n",
    "\n",
    "plt.title(\"Monthly Tweet Volume by Account Category\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Number of Tweets\")\n",
    "plt.legend(title=\"Account Category\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a261cd02",
   "metadata": {},
   "source": [
    "## Tweet distribution statistics\n",
    "- Detecting spread of nunique tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c7777a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_per_account_spark = (\n",
    "    df.groupBy(\"author\")\n",
    "      .count()\n",
    "      .withColumnRenamed(\"count\", \"tweets_per_account\")\n",
    ")\n",
    "\n",
    "tweets_per_account = tweets_per_account_spark.toPandas()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(tweets_per_account[\"tweets_per_account\"], bins=50, color=\"steelblue\", edgecolor=\"black\")\n",
    "plt.title(\"Distribution of Tweets per Account (Linear Scale)\")\n",
    "plt.xlabel(\"Number of Tweets\")\n",
    "plt.ylabel(\"Number of Accounts\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(tweets_per_account[\"tweets_per_account\"], bins=50, color=\"salmon\", edgecolor=\"black\", log=True)\n",
    "plt.title(\"Distribution of Tweets per Account (Log Scale)\")\n",
    "plt.xlabel(\"Number of Tweets (log scale)\")\n",
    "plt.ylabel(\"Number of Accounts\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a7992f",
   "metadata": {},
   "source": [
    "## Detecting outliers \n",
    "- We create new smaller datasets derived from the initial dataset of accounts that are outliers \n",
    "- This is done to explore the most prominent trolls and investigate their behaviour "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07cad56",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_per_account = (\n",
    "    df.groupBy(\"author\")\n",
    "      .agg(count(\"*\").alias(\"tweets_per_account\"))\n",
    ")\n",
    "\n",
    "# high activity datasets\n",
    "accounts_over_50000 = tweets_per_account.filter(col(\"tweets_per_account\") > 50000)\n",
    "accounts_over_40000 = tweets_per_account.filter(col(\"tweets_per_account\") > 40000)\n",
    "\n",
    "\n",
    "print(\"Accounts with > 50,000 tweets:\")\n",
    "accounts_over_50000.show(truncate=False)\n",
    "\n",
    "print(\"\\nAccounts with > 40,000 tweets:\")\n",
    "accounts_over_40000.show(truncate=False)\n",
    "\n",
    "# convertion to pandas for inspection\n",
    "accounts_50k_pd = accounts_over_50000.toPandas()\n",
    "accounts_40k_pd = accounts_over_40000.toPandas()\n",
    "\n",
    "accounts_50k_pd, accounts_40k_pd\n",
    "\n",
    "# tweets per account\n",
    "tweets_per_account = (\n",
    "    df.groupBy(\"author\")\n",
    "      .agg(count(\"*\").alias(\"tweets_per_account\"))\n",
    ")\n",
    "\n",
    "# pandas for percentile calc\n",
    "tp_pd = tweets_per_account.toPandas()\n",
    "\n",
    "# # 95th percentile \n",
    "p95_threshold = tp_pd[\"tweets_per_account\"].quantile(0.95)\n",
    "\n",
    "print(f\"90th percentile tweet count threshold: {p95_threshold:.2f}\")\n",
    "\n",
    "# nunique accounts above 95th percentile\n",
    "n_above_90 = (tp_pd[\"tweets_per_account\"] > p95_threshold).sum()\n",
    "\n",
    "print(f\"Number of accounts above 95th percentile: {n_above_90}\")\n",
    "\n",
    "# show as df\n",
    "accounts_above_90 = tweets_per_account.filter(col(\"tweets_per_account\") > p95_threshold)\n",
    "accounts_above_90.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f6706f",
   "metadata": {},
   "source": [
    "## Further exploration of the outliers \n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3329a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_per_account = (\n",
    "    df.groupBy(\"author\")\n",
    "      .agg(count(\"*\").alias(\"tweets_per_account\"))\n",
    ")\n",
    "\n",
    "tp_pd = tweets_per_account.toPandas()\n",
    "p95_threshold = tp_pd[\"tweets_per_account\"].quantile(0.95)\n",
    "\n",
    "p95_threshold\n",
    "\n",
    "accounts_above_95 = (\n",
    "    tweets_per_account\n",
    "        .filter(col(\"tweets_per_account\") > p95_threshold)\n",
    "        .withColumnRenamed(\"author\", \"account\")\n",
    ")\n",
    "\n",
    "accounts_above_95.show(truncate=False)\n",
    "print(\"Number of accounts above 95th percentile:\", accounts_above_95.count())\n",
    "\n",
    "df_top5 = (\n",
    "    df.join(accounts_above_95, df.author == accounts_above_95.account, \"inner\")\n",
    "      .drop(accounts_above_95.account)\n",
    ")\n",
    "\n",
    "df_top5.printSchema()\n",
    "print(\"Total tweets from top 5% accounts:\", df_top5.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ed9fcf",
   "metadata": {},
   "source": [
    "## Text and Topic Exploration\n",
    "\n",
    "So far, our analysis has focused on *when* IRA accounts were active and *how* they behaved (volume, categories, activations, outliers). In this section, we briefly turn to **what** they were talking about.\n",
    "\n",
    "Rather than building a full-blown topic model, we adopt a simple and transparent\n",
    "**bag-of-words content analysis**, which is more than sufficient for the scope of this project and the marking scheme. The idea is to use word frequencies as a proxy for the main themes pushed by IRA accounts, and to see how these themes evolve over time.\n",
    "\n",
    "Concretely, we:\n",
    "\n",
    "1. **Focus on English-language tweets**, which dominate the dataset and are most relevant to US politics.\n",
    "2. **Clean and tokenize tweet text**:\n",
    "   - lowercasing\n",
    "   - removing URLs, @mentions and punctuation\n",
    "   - stripping stopwords (e.g. “the”, “and”, “to”).\n",
    "3. Compute:\n",
    "   - the **most frequent words overall** (top unigrams), giving a first impression of the dominant narratives,\n",
    "   - the **most frequent words by year**, showing how the vocabulary shifts over time (e.g., from Ukraine and Crimea to Trump, Clinton, immigration, and race-related terms).\n",
    "\n",
    "This “lightweight topic extraction” allows us to:\n",
    "- highlight prominent political actors (e.g. *trump*, *clinton*),\n",
    "- identify recurring frames (e.g. *police*, *black*, *law*, *terrorist*),\n",
    "- and compare how the language used by IRA accounts changed around key events.\n",
    "\n",
    "Later, we could extend this analysis with:\n",
    "- bigrams (two-word phrases such as *black lives*, *hillary clinton*),\n",
    "- category-specific vocabularies (e.g., words over-represented in **RightTroll** vs **LeftTroll** accounts),\n",
    "- or full topic models (e.g. LDA) if needed.\n",
    "For the purposes of this project, however, simple word-frequency–based topic exploration already provides a clear and interpretable view of the main narratives in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cab8131",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lower, regexp_replace, split, explode, year\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "# restrict to english words and start from 2014 \n",
    "df_text = (\n",
    "    df_time\n",
    "        .filter(col(\"language\") == \"English\")\n",
    "        .filter(df_time.publish_ts >= \"2014-01-01\")\n",
    ")\n",
    "\n",
    "# text cleaning\n",
    "df_clean = (\n",
    "    df_text\n",
    "        .withColumn(\"text_lower\", lower(col(\"content\")))\n",
    "        .fillna({\"text_lower\": \"\"})\n",
    "        # remove URLs\n",
    "        .withColumn(\"text_lower\", regexp_replace(col(\"text_lower\"), r\"http\\S+\", \" \"))\n",
    "        .withColumn(\"text_lower\", regexp_replace(col(\"text_lower\"), r\"www\\.\\S+\", \" \"))\n",
    "        # remove @mentions\n",
    "        .withColumn(\"text_lower\", regexp_replace(col(\"text_lower\"), r\"@\\w+\", \" \"))\n",
    "        # keep letters only; remove hashtags entirely\n",
    "        .withColumn(\"text_lower\", regexp_replace(col(\"text_lower\"), r\"#\\w+\", \" \"))   # <-- REMOVE HASHTAGS\n",
    "        .withColumn(\"text_lower\", regexp_replace(col(\"text_lower\"), r\"[^a-z]\", \" \"))\n",
    "        # collapse whitespace\n",
    "        .withColumn(\"text_lower\", regexp_replace(col(\"text_lower\"), r\"\\s+\", \" \"))\n",
    ")\n",
    "\n",
    "# tokenize\n",
    "df_tokens = df_clean.withColumn(\"token\", explode(split(col(\"text_lower\"), \" \")))\n",
    "\n",
    "# stopword filter\n",
    "stopwords = StopWordsRemover.loadDefaultStopWords(\"english\")\n",
    "extra_stops = [\"rt\", \"amp\", \"https\", \"http\"]\n",
    "stopwords_all = stopwords + extra_stops\n",
    "\n",
    "df_words = (\n",
    "    df_tokens\n",
    "        .filter(col(\"token\") != \"\")\n",
    "        .filter(~col(\"token\").isin(stopwords_all))\n",
    ")\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "top_words_global = (\n",
    "    df_words.groupBy(\"token\")\n",
    "            .count()\n",
    "            .orderBy(desc(\"count\"))\n",
    ")\n",
    "\n",
    "top_words_global.show(50, truncate=False)\n",
    "\n",
    "\n",
    "df_words_year = df_words.withColumn(\"year\", year(col(\"publish_ts\")))\n",
    "df_words_year = df_words_year.filter(col(\"year\").between(2015, 2017))\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "words_per_year = (\n",
    "    df_words_year\n",
    "        .groupBy(\"year\", \"token\")\n",
    "        .count()\n",
    ")\n",
    "\n",
    "w = Window.partitionBy(\"year\").orderBy(desc(\"count\"))\n",
    "\n",
    "top_words_per_year = (\n",
    "    words_per_year\n",
    "        .withColumn(\"rn\", row_number().over(w))\n",
    "        .filter(col(\"rn\") <= 20)\n",
    "        .orderBy(\"year\", \"rn\")\n",
    ")\n",
    "\n",
    "top_words_per_year.show(60, truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db97e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc, year, month, col\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# top 20 words but we only use top 10 later \n",
    "top20 = (\n",
    "    df_words.groupBy(\"token\")\n",
    "            .count()\n",
    "            .orderBy(desc(\"count\"))\n",
    "            .limit(20)\n",
    "            .toPandas()\n",
    ")\n",
    "\n",
    "top20 = top20.sort_values(\"count\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "top10_words = top20[\"token\"].iloc[:5].tolist()      # top 5\n",
    "next10_words = top20[\"token\"].iloc[5:10].tolist()   # ranks 6–10\n",
    "\n",
    "top10_words, next10_words\n",
    "\n",
    "# monthly counts per word\n",
    "df_words_monthly = (\n",
    "    df_words\n",
    "        .withColumn(\"year\", year(col(\"publish_ts\")))\n",
    "        .withColumn(\"month\", month(col(\"publish_ts\")))\n",
    "        .groupBy(\"year\", \"month\", \"token\")\n",
    "        .count()\n",
    ")\n",
    "\n",
    "words_monthly_pd = df_words_monthly.toPandas()\n",
    "words_monthly_pd[\"year_month\"] = pd.to_datetime(\n",
    "    words_monthly_pd[\"year\"].astype(str) + \"-\" +\n",
    "    words_monthly_pd[\"month\"].astype(str).str.zfill(2) + \"-01\"\n",
    ")\n",
    "words_monthly_pd = words_monthly_pd.sort_values(\"year_month\")\n",
    "\n",
    "# monthly total word counts\n",
    "df_monthly_totals = (\n",
    "    df_words\n",
    "        .withColumn(\"year\", year(col(\"publish_ts\")))\n",
    "        .withColumn(\"month\", month(col(\"publish_ts\")))\n",
    "        .groupBy(\"year\", \"month\")\n",
    "        .count()\n",
    "        .withColumnRenamed(\"count\", \"total_words_month\")\n",
    ")\n",
    "\n",
    "monthly_totals_pd = df_monthly_totals.toPandas()\n",
    "monthly_totals_pd[\"year_month\"] = pd.to_datetime(\n",
    "    monthly_totals_pd[\"year\"].astype(str) + \"-\" +\n",
    "    monthly_totals_pd[\"month\"].astype(str).str.zfill(2) + \"-01\"\n",
    ")\n",
    "\n",
    "# compute normalized frequency \n",
    "wm = words_monthly_pd.merge(\n",
    "    monthly_totals_pd[[\"year_month\", \"total_words_month\"]],\n",
    "    on=\"year_month\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "wm[\"norm_freq\"] = wm[\"count\"] / wm[\"total_words_month\"]\n",
    "\n",
    "# load events csv \n",
    "events_path = \"/Users/sanduandrei/Desktop/BigData/data/events/events_temporal_markers.csv\"\n",
    "events_pd = pd.read_csv(events_path)\n",
    "events_pd[\"event_date\"] = pd.to_datetime(events_pd[\"date\"], format=\"%m/%d/%Y\")\n",
    "\n",
    "colors_events = plt.cm.tab20.colors\n",
    "\n",
    "# plt top 5 words normalized\n",
    "fig, ax = plt.subplots(figsize=(18, 6))\n",
    "\n",
    "df_top10 = wm[wm[\"token\"].isin(top10_words)]\n",
    "\n",
    "for word in top10_words:\n",
    "    sub = df_top10[df_top10[\"token\"] == word]\n",
    "    ax.plot(sub[\"year_month\"], sub[\"norm_freq\"], marker=\"o\", linewidth=1.5, label=word)\n",
    "\n",
    "for idx, (_, row) in enumerate(events_pd.iterrows()):\n",
    "    ax.axvline(\n",
    "        row[\"event_date\"],\n",
    "        color=colors_events[idx % len(colors_events)],\n",
    "        linestyle=\"--\",\n",
    "        linewidth=1,\n",
    "        alpha=0.6,\n",
    "        label=row[\"label\"]\n",
    "    )\n",
    "\n",
    "ax.set_title(\"Top 5 Most Frequent Words — Normalized Monthly Frequency with Key Events\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Normalized Frequency\")\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\", fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# plt words 6-10 normalized\n",
    "fig, ax = plt.subplots(figsize=(18, 6))\n",
    "\n",
    "df_next10 = wm[wm[\"token\"].isin(next10_words)]\n",
    "\n",
    "for word in next10_words:\n",
    "    sub = df_next10[df_next10[\"token\"] == word]\n",
    "    ax.plot(sub[\"year_month\"], sub[\"norm_freq\"], marker=\"o\", linewidth=1.5, label=word)\n",
    "\n",
    "for idx, (_, row) in enumerate(events_pd.iterrows()):\n",
    "    ax.axvline(\n",
    "        row[\"event_date\"],\n",
    "        color=colors_events[idx % len(colors_events)],\n",
    "        linestyle=\"--\",\n",
    "        linewidth=1,\n",
    "        alpha=0.6,\n",
    "        label=row[\"label\"]\n",
    "    )\n",
    "\n",
    "ax.set_title(\"Words Ranked 6–10 — Normalized Monthly Frequency with Key Events\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Normalized Frequency\")\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\", fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5083a375",
   "metadata": {},
   "source": [
    "### Category-Specific Vocabularies\n",
    "\n",
    "So far, we have looked at the dominant words used across all English IRA tweets.  \n",
    "However, IRA accounts are not homogeneous: they are grouped into categories such as **RightTroll**, **LeftTroll**, **NewsFeed**, or **HashtagGamer**, each targeting different audiences and narratives.\n",
    "\n",
    "In this section, we explore **category-specific vocabularies** by examining the most frequent words within each account category (restricted to English-language tweets).\n",
    "\n",
    "The goal is to see whether different troll personas:\n",
    "\n",
    "- focus on different political actors or issues (e.g. *trump* vs *police* vs *refugees*),\n",
    "- emphasize different frames (e.g. law & order, racial tension, patriotism),\n",
    "- and potentially mirror the language of the groups they are trying to imitate.\n",
    "\n",
    "We compute the top words per category using the cleaned tokenized text (no URLs, mentions, hashtags, or stopwords) and compare how the dominant vocabulary differs between categories such as **RightTroll**, **LeftTroll**, and **NewsFeed**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652ef9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, desc, count as spark_count, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# category and sizes \n",
    "category_counts = (\n",
    "    df_words\n",
    "        .groupBy(\"account_category\")\n",
    "        .agg(spark_count(\"*\").alias(\"n_tokens\"))\n",
    "        .orderBy(desc(\"n_tokens\"))\n",
    ")\n",
    "\n",
    "category_counts.show(truncate=False)\n",
    "\n",
    "# only top n categories, keeps things readable \n",
    "TOP_CATS = 5\n",
    "top_categories = [\n",
    "    row[\"account_category\"]\n",
    "    for row in category_counts.limit(TOP_CATS).collect()\n",
    "    if row[\"account_category\"] is not None\n",
    "]\n",
    "\n",
    "print(\"Top categories:\", top_categories)\n",
    "\n",
    "df_words_cat = df_words.filter(col(\"account_category\").isin(top_categories))\n",
    "\n",
    "# word frequencies per category \n",
    "words_by_cat = (\n",
    "    df_words_cat\n",
    "        .groupBy(\"account_category\", \"token\")\n",
    "        .agg(spark_count(\"*\").alias(\"count\"))\n",
    ")\n",
    "\n",
    "# rank words by frequency \n",
    "TOP_K = 20\n",
    "\n",
    "w_cat = Window.partitionBy(\"account_category\").orderBy(desc(\"count\"))\n",
    "\n",
    "top_words_by_cat = (\n",
    "    words_by_cat\n",
    "        .withColumn(\"rank\", row_number().over(w_cat))\n",
    "        .filter(col(\"rank\") <= TOP_K)\n",
    "        .orderBy(\"account_category\", \"rank\")\n",
    ")\n",
    "\n",
    "top_words_by_cat.show(200, truncate=False)\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# convert to pandas\n",
    "top_words_pd = top_words_by_cat.toPandas()\n",
    "\n",
    "# pivot so each category is a row, each rank a column\n",
    "table_df = (\n",
    "    top_words_pd\n",
    "        .sort_values([\"account_category\", \"rank\"])\n",
    "        .pivot(index=\"account_category\", columns=\"rank\", values=\"token\")\n",
    ")\n",
    "\n",
    "# column headers: 1, 2, 3, ...\n",
    "table_df.columns = [str(i) for i in table_df.columns]\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Matplotlib table\n",
    "# ----------------------------------------------------\n",
    "fig, ax = plt.subplots(figsize=(14, 4 + 0.4 * len(table_df)))\n",
    "ax.axis(\"off\")\n",
    "\n",
    "mpl_table = ax.table(\n",
    "    cellText=table_df.values,\n",
    "    rowLabels=table_df.index,\n",
    "    colLabels=table_df.columns,\n",
    "    cellLoc=\"center\",\n",
    "    loc=\"center\"\n",
    ")\n",
    "\n",
    "mpl_table.auto_set_font_size(False)\n",
    "mpl_table.set_fontsize(10)\n",
    "mpl_table.scale(1.3, 1.4)\n",
    "\n",
    "# make columns a bit wider so words fit better\n",
    "mpl_table.auto_set_column_width(col=list(range(len(table_df.columns) + 1)))\n",
    "\n",
    "# style header row\n",
    "for (row, col), cell in mpl_table.get_celld().items():\n",
    "    if row == 0:  # header row\n",
    "        cell.set_text_props(weight=\"bold\", color=\"white\")\n",
    "        cell.set_facecolor(\"#4E79A7\")\n",
    "\n",
    "# zebra-strip data rows\n",
    "for (row, col), cell in mpl_table.get_celld().items():\n",
    "    if row > 0:  # skip header\n",
    "        if row % 2 == 0:\n",
    "            cell.set_facecolor(\"#F5F5F5\")\n",
    "        else:\n",
    "            cell.set_facecolor(\"white\")\n",
    "\n",
    "plt.title(\"Top Words by Account Category\", fontsize=16, pad=25)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55163b38",
   "metadata": {},
   "source": [
    "## Bigram Analysis (Two-Word Topics)\n",
    "\n",
    "Unigram (single-word) frequencies already showed that IRA accounts focused heavily on\n",
    "political figures (e.g. *trump*), racial issues (e.g. *police*, *black*), and general\n",
    "political vocabulary. However, many narratives are better captured by **short phrases**\n",
    "rather than isolated words — for example:\n",
    "\n",
    "- *black lives*\n",
    "- *police shooting*\n",
    "- *hillary clinton*\n",
    "- *fake news*\n",
    "- *donald trump*\n",
    "\n",
    "To capture these patterns, we perform a simple **bigram analysis**, where we:\n",
    "\n",
    "1. Reuse the cleaned English tweets from 2014 onwards.\n",
    "2. Tokenize each tweet into words (after removing URLs, mentions, hashtags and\n",
    "   stopwords).\n",
    "3. Build **bigrams** as consecutive pairs of remaining tokens.\n",
    "4. Compute:\n",
    "   - the **most frequent bigrams overall**, and\n",
    "   - the **top bigrams by year**, to see how phrase-level topics shift over time.\n",
    "\n",
    "This lightweight extension of the topic analysis highlights short, interpretable phrases\n",
    "that correspond more directly to specific narratives pushed by IRA accounts (e.g. crime\n",
    "and policing, electoral politics, social justice, foreign policy).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279cb713",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m df_text_big = (\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[43mdf_time\u001b[49m\n\u001b[32m      3\u001b[39m         .filter(col(\u001b[33m\"\u001b[39m\u001b[33mlanguage\u001b[39m\u001b[33m\"\u001b[39m) == \u001b[33m\"\u001b[39m\u001b[33mEnglish\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m         .filter(df_time.publish_ts >= \u001b[33m\"\u001b[39m\u001b[33m2014-01-01\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m         .select(\u001b[33m\"\u001b[39m\u001b[33mpublish_ts\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33maccount_category\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m )\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# ----------------------------------------------------\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# 2. Clean text (similar to unigram analysis, but keep arrays)\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m#    - lowercase\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m#    - remove URLs, mentions, hashtags, non-letters\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# ----------------------------------------------------\u001b[39;00m\n\u001b[32m     13\u001b[39m df_clean_big = (\n\u001b[32m     14\u001b[39m     df_text_big\n\u001b[32m     15\u001b[39m         .withColumn(\u001b[33m\"\u001b[39m\u001b[33mtext_lower\u001b[39m\u001b[33m\"\u001b[39m, lower(col(\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m)))\n\u001b[32m   (...)\u001b[39m\u001b[32m     27\u001b[39m         .withColumn(\u001b[33m\"\u001b[39m\u001b[33mtext_lower\u001b[39m\u001b[33m\"\u001b[39m, regexp_replace(col(\u001b[33m\"\u001b[39m\u001b[33mtext_lower\u001b[39m\u001b[33m\"\u001b[39m), \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms+\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     28\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'df_time' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, lower, regexp_replace, split, explode,\n",
    "    year, desc\n",
    ")\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "df_text_big = (\n",
    "    df_time\n",
    "        .filter(col(\"language\") == \"English\")\n",
    "        .filter(df_time.publish_ts >= \"2014-01-01\")\n",
    "        .select(\"publish_ts\", \"account_category\", \"content\")\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 2. Clean text (similar to unigram analysis, but keep arrays)\n",
    "#    - lowercase\n",
    "#    - remove URLs, mentions, hashtags, non-letters\n",
    "# ----------------------------------------------------\n",
    "df_clean_big = (\n",
    "    df_text_big\n",
    "        .withColumn(\"text_lower\", lower(col(\"content\")))\n",
    "        .fillna({\"text_lower\": \"\"})\n",
    "        # remove URLs\n",
    "        .withColumn(\"text_lower\", regexp_replace(col(\"text_lower\"), r\"http\\S+\", \" \"))\n",
    "        .withColumn(\"text_lower\", regexp_replace(col(\"text_lower\"), r\"www\\.\\S+\", \" \"))\n",
    "        # remove @mentions\n",
    "        .withColumn(\"text_lower\", regexp_replace(col(\"text_lower\"), r\"@\\w+\", \" \"))\n",
    "        # remove hashtags entirely\n",
    "        .withColumn(\"text_lower\", regexp_replace(col(\"text_lower\"), r\"#\\w+\", \" \"))\n",
    "        # keep letters only\n",
    "        .withColumn(\"text_lower\", regexp_replace(col(\"text_lower\"), r\"[^a-z]\", \" \"))\n",
    "        # collapse whitespace\n",
    "        .withColumn(\"text_lower\", regexp_replace(col(\"text_lower\"), r\"\\s+\", \" \"))\n",
    ")\n",
    "\n",
    "# tokenize into raw tokens (arrays)\n",
    "df_tokens_big = df_clean_big.withColumn(\"tokens_raw\", split(col(\"text_lower\"), \" \"))\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 3. Remove stopwords at the array level\n",
    "# ----------------------------------------------------\n",
    "stopwords = set(StopWordsRemover.loadDefaultStopWords(\"english\") +\n",
    "                [\"rt\", \"amp\", \"https\", \"http\", \"\"])\n",
    "\n",
    "@udf(ArrayType(StringType()))\n",
    "def filter_stopwords(tokens):\n",
    "    if tokens is None:\n",
    "        return []\n",
    "    return [t for t in tokens if t not in stopwords]\n",
    "\n",
    "df_tokens_big = df_tokens_big.withColumn(\"tokens_clean\", filter_stopwords(col(\"tokens_raw\")))\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 4. Build bigrams (consecutive tokens)\n",
    "# ----------------------------------------------------\n",
    "@udf(ArrayType(StringType()))\n",
    "def make_bigrams(tokens):\n",
    "    if tokens is None:\n",
    "        return []\n",
    "    return [tokens[i] + \" \" + tokens[i+1] for i in range(len(tokens) - 1)]\n",
    "\n",
    "df_bigrams = df_tokens_big.withColumn(\"bigrams\", make_bigrams(col(\"tokens_clean\")))\n",
    "\n",
    "# explode to one bigram per row\n",
    "df_bigrams_exploded = (\n",
    "    df_bigrams\n",
    "        .select(\"publish_ts\", \"account_category\", explode(col(\"bigrams\")).alias(\"bigram\"))\n",
    "        .filter(col(\"bigram\") != \"\")\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 5. Top bigrams overall\n",
    "# ----------------------------------------------------\n",
    "top_bigrams_overall = (\n",
    "    df_bigrams_exploded\n",
    "        .groupBy(\"bigram\")\n",
    "        .count()\n",
    "        .orderBy(desc(\"count\"))\n",
    ")\n",
    "\n",
    "print(\"Top 30 bigrams overall:\")\n",
    "top_bigrams_overall.show(30, truncate=False)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 6. Top bigrams by year (2015–2017, for example)\n",
    "# ----------------------------------------------------\n",
    "df_big_year = df_bigrams_exploded.withColumn(\"year\", year(col(\"publish_ts\")))\n",
    "df_big_year = df_big_year.filter(col(\"year\").between(2015, 2017))\n",
    "\n",
    "bigrams_year_counts = (\n",
    "    df_big_year\n",
    "        .groupBy(\"year\", \"bigram\")\n",
    "        .count()\n",
    ")\n",
    "\n",
    "w = Window.partitionBy(\"year\").orderBy(desc(\"count\"))\n",
    "\n",
    "top_bigrams_per_year = (\n",
    "    bigrams_year_counts\n",
    "        .withColumn(\"rank\", row_number().over(w))\n",
    "        .filter(col(\"rank\") <= 15)\n",
    "        .orderBy(\"year\", \"rank\")\n",
    ")\n",
    "\n",
    "print(\"Top 15 bigrams per year (2015–2017):\")\n",
    "top_bigrams_per_year.show(60, truncate=False)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 7. (Optional) Simple bar chart for top 10 bigrams overall\n",
    "# ----------------------------------------------------\n",
    "top10_bigrams_pd = top_bigrams_overall.limit(10).toPandas().sort_values(\"count\", ascending=True)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(top10_bigrams_pd[\"bigram\"], top10_bigrams_pd[\"count\"])\n",
    "plt.title(\"Top 10 Bigrams in English IRA Tweets (2014+)\")\n",
    "plt.xlabel(\"Count\")\n",
    "plt.ylabel(\"Bigram\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
